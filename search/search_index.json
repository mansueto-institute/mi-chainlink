{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"chainlink","text":"<p>A powerful, flexible framework for entity resolution and record linkage using DuckDB as the database engine built upon the work of Who Owns Chicago (public release Summer 2025) by the Mansueto Institute for Urban Innovation including the work of Kevin Bryson, Ana (Anita) Restrepo Lachman, Caitlin P., Joaquin Pinto, and Divij Sinha. </p> <p>This package enables you to load data from various sources, clean and standardize entity names and addresses, and create links between entities based on exact and fuzzy matching techniques.</p> <p>Source: https://github.com/mansueto-institute/mi-chainlink</p> <p>Documentation: https://mansueto-institute.github.io/mi-chainlink/</p> <p>Issues: https://github.com/mansueto-institute/mi-chainlink/issues</p>"},{"location":"#overview","title":"Overview","text":"<p>This framework helps you solve the entity resolution problem by:</p> <ol> <li>Loading data from multiple sources into a DuckDB database</li> <li>Cleaning and standardizing entity names and addresses</li> <li>Creating exact matches between entities based on names and addresses</li> <li>Generating fuzzy matches using TF-IDF similarity on names and addresses</li> <li>Exporting the resulting linked data for further analysis</li> </ol> <p>The system is designed to be configurable through YAML files and supports incremental updates to an existing database.</p>"},{"location":"#installation","title":"Installation","text":"<p>Package is available on PyPI. You can install it using pip or uv:</p> <pre><code>pip install chainlink\n</code></pre> <pre><code>uv add chainlink\n</code></pre>"},{"location":"#usage","title":"Usage","text":""},{"location":"#command-line-interface","title":"Command Line Interface","text":"<pre><code># precongire YAML config file and run using config\nchainlink [&lt;path_to_config_file&gt;]\n\n# or run config creater\nchainlink\n</code></pre>"},{"location":"#configuration","title":"Configuration","text":"<p>The framework uses YAML configuration files to define data sources and linking parameters. See a template file in <code>src/chainlink/configs/config_template.yaml</code>. Here's a template configuration. See an example config at <code>src/chainlink/configs/woc_config_sample.yaml</code> :</p> <pre><code>options:\n  bad_address_path: data/bad_addresses.csv # path to a csv file with bad addresses that should not be matched\n  export_tables: true # bool whether to export the tables to parquet files\n  db_path: data/link.db # path to the resulting DuckDB database\n  overwrite_db: true # whether to force overwrite the existing database or add to existing tables\n  link_exclusions: # can specify exclusions for the matching process\n  update_config_only: false # whether to update the config only\n  load_only: false # whether to only load the data without matching\n  probabilistic: true # whether to use probabilistic matching for name and address\nschemas:\n  - schema_name: schema1 # name of the schema\n    tables:\n      - address_cols:\n          - address # address column\n        id_col: file_num1 # id column\n        name_cols:\n          - name_raw # name column\n        table_name: table1 # name of the table\n        table_name_path: data/import/schema1_table1.parquet # path to the table\n      - address_cols:\n          - address2 # address column\n        id_col: file_num2 # id column\n        name_cols:\n          - name2 # name column\n        table_name: table2 # name of the table\n        table_name_path: data/import/schema1_table2.parquet # path to the table\n  - schema_name: schema2 # name of the schema\n    tables:\n      - address_cols:\n          - mailing_address # address column\n          - property_address # address column\n        id_col: pin # id column\n        name_cols:\n          - tax_payer_name # name column\n        table_name: table1 # name of the table\n        table_name_path: data/schema2_table1.parquet # path to the table\nmetadata:\n  existing_links:\n  last_updated: # date of the last update\n\n</code></pre>"},{"location":"#interactive-configuration","title":"Interactive Configuration","text":"<p>If you don't have a configuration file, the system will guide you through creating one:</p> <pre><code>&gt; Enter config path. [Leave blank if you would you like to create a new one]\n\n&gt; Enter the path to the resulting database [db/linked.db]:\ndata/property_linkage.db\n\n&gt; Only clean and load data to the database (without matching)? [y/N]: n\n\n&gt; Run probabilisitic name and address matching? [y/N]: y\n\n&gt; Export tables to parquet after load? [y/N]: y\n\n&gt; [Optional] Provide path to bad address csv file\ndata/bad_addresses.csv\n\n&gt; Add a new schema? [Y/n]: y\n\n&gt; Enter the name of the schema [main]: property\n\n&gt; Enter the name of dataset: [dataset]: assessor\n&gt; Enter the path to the dataset: data/assessor_data.csv\n&gt; Enter the id column of the dataset. Must be unique: pin\n&gt; Enter the name column(s) (comma separated): taxpayer_name, owner_name\n&gt; Enter the address column(s) (comma separated): property_address, mailing_address\n\n&gt; Add another table to this schema? [y/N]: y\n\n&gt; Enter the name of dataset: [dataset]: permits\n&gt; Enter the path to the dataset: data/permit_records.parquet\n&gt; Enter the id column of the dataset. Must be unique: permit_id\n&gt; Enter the name column(s) (comma separated): applicant_name\n&gt; Enter the address column(s) (comma separated): property_address\n\n&gt; Add another table to this schema? [y/N]: n\n\n&gt; Schema added successfully!\n\n&gt; Add another schema? [y/N]: y\n\n&gt; Enter the name of the schema [main]: businesses\n&gt; Enter the name of dataset: [dataset]: licenses\n&gt; Enter the path to the dataset: data/business_licenses.csv\n&gt; Enter the id column of the dataset. Must be unique: license_num\n&gt; Enter the name column(s) (comma separated): business_name, dba_name\n&gt; Enter the address column(s) (comma separated): address\n\n&gt; Add another table to this schema? [y/N]: n\n\n&gt; Schema added successfully!\n\n&gt; Add another schema? [y/N]: n\n</code></pre>"},{"location":"#detailed-process","title":"Detailed Process","text":""},{"location":"#1-loading-data","title":"1. Loading Data","text":"<p>The framework loads data from CSV or Parquet files as specified in the configuration. For each table:</p> <ul> <li>Validates required columns exist</li> <li>Converts column names to snake_case</li> <li>Cleans entity names and addresses</li> <li>Creates unique IDs for names, addresses, streets, and street names</li> <li>Loads data into the specified schema in the DuckDB database</li> </ul>"},{"location":"#2-creating-links","title":"2. Creating Links","text":""},{"location":"#exact-matching","title":"Exact Matching","text":"<p>The framework creates exact matches between entities based on:</p> <ul> <li>Name matches: Exact string matches between name fields</li> <li>Address matches:<ul> <li>Raw address string matches</li> <li>Street matches</li> <li>Unit matches (when street matches)</li> <li>Street name and number matches (when zip code matches)</li> </ul> </li> </ul>"},{"location":"#fuzzy-matching","title":"Fuzzy Matching","text":"<p>If enabled, the framework also creates fuzzy matches using TF-IDF:</p> <ul> <li>Names</li> <li>Generates TF-IDF vectors for all entity names</li> <li>Computes similarity scores between entities</li> <li>Stores matches above a threshold (0.8 by default)</li> <li>Addresses</li> <li>Generates TF-IDF vectors for all entity addresses using <code>number + direction + fuzzy(street_name) + street_post_type</code></li> <li>Computes similarity scores between entities</li> <li>Stores matches above a threshold (0.8 by default)</li> </ul>"},{"location":"#3-exporting-results","title":"3. Exporting Results","text":"<p>If configured, the framework exports all tables to Parquet files in <code>data/export/</code> directory.</p>"},{"location":"advanced-features/","title":"Advanced Features","text":""},{"location":"advanced-features/#bad-address-handling","title":"Bad Address Handling","text":"<p>The framework can exclude known bad addresses from matching by adding file paths to the configuration. This is useful for filtering out addresses that are known to be incorrect or problematic. The file should be a CSV with a header row with the first column containing the bad addresses.</p> <pre><code>options:\n  bad_address_path: data/bad_addresses.csv\n  ...\n</code></pre>"},{"location":"advanced-features/#link-exclusions","title":"Link Exclusions","text":"<p>You can exclude specific types of links from being created in the database. This is useful for filtering out certain types of matches that may not be relevant to your analysis. Include the link types you want to exclude in the configuration file.</p> <p>```yaml: options:   link_exclusions:   - exclude_link_1   - exclude_link_2   ...</p> <pre><code>\n### Incremental Updates\n\nThe framework supports incremental updates to an existing database. Change `overwrite_db` option to `false` in the configuration file. This allows you to add new data to the database without overwriting existing data.\n\n```yaml:\n\noptions:\n  overwrite_db: false\n  ...\n</code></pre>"},{"location":"contributing/","title":"Contributing to <code>chainlink</code>","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/mansueto-institute/mi-chainlink/issues</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement a fix for it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>chainlink could always use more documentation, whether as part of the official docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/mansueto-institute/mi-chainlink/issues.</p> <p>If you are proposing a new feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions   are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up <code>chainlink</code> for local development. Please note this documentation assumes you already have <code>uv</code> and <code>Git</code> installed and ready to go.</p> <ol> <li> <p>Fork the <code>chainlink</code> repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> </li> </ol> <pre><code>cd &lt;directory_in_which_repo_should_be_created&gt;\ngit clone git@github.com:YOUR_NAME/mi-chainlink.git\n</code></pre> <ol> <li>Now we need to install the environment. Navigate into the directory</li> </ol> <pre><code>cd chainlink\n</code></pre> <p>Then, install and activate the environment with:</p> <pre><code>uv sync\n</code></pre> <ol> <li>Install pre-commit to run linters/formatters at commit time:</li> </ol> <pre><code>uv run pre-commit install\n</code></pre> <ol> <li>Create a branch for local development:</li> </ol> <pre><code>git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> <ol> <li> <p>Don't forget to add test cases for your added functionality to the <code>tests</code> directory.</p> </li> <li> <p>When you're done making changes, check that your changes pass the formatting tests.</p> </li> </ol> <pre><code>make check\n</code></pre> <p>Now, validate that all unit tests are passing:</p> <pre><code>make test\n</code></pre> <ol> <li>Before raising a pull request you should also run tox.    This will run the tests across different versions of Python:</li> </ol> <pre><code>tox\n</code></pre> <p>This requires you to have multiple versions of python installed. This step is also triggered in the CI/CD pipeline, so you could also choose to skip this step locally.</p> <ol> <li>Commit your changes and push your branch to GitHub:</li> </ol> <pre><code>git add .\ngit commit -m \"Your detailed description of your changes.\"\ngit push origin name-of-your-bugfix-or-feature\n</code></pre> <ol> <li>Submit a pull request through the GitHub website.</li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li> <p>The pull request should include tests.</p> </li> <li> <p>If the pull request adds functionality, the docs should be updated.    Put your new functionality into a function with a docstring, and add the feature to the list in <code>README.md</code>.</p> </li> </ol>"},{"location":"modules/","title":"API","text":""},{"location":"modules/#chainlink.cleaning","title":"<code>cleaning</code>","text":""},{"location":"modules/#chainlink.cleaning.cleaning_functions","title":"<code>cleaning_functions</code>","text":""},{"location":"modules/#chainlink.cleaning.cleaning_functions.clean_address","title":"<code>clean_address(raw)</code>","text":"<p>Given a raw address, first conduct baseline address cleaning, then identify and return the components of the address. Where possible, infer correct city and state value from the zip code given.</p> <p>Parameters:</p> Name Type Description Default <code>raw</code> <code>str</code> <p>A raw address.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of address components.</p> Source code in <code>src/chainlink/cleaning/cleaning_functions.py</code> <pre><code>def clean_address(raw: str) -&gt; dict:\n    \"\"\"\n    Given a raw address, first conduct baseline address cleaning, then\n    identify and return the components of the address. Where possible, infer\n    correct city and state value from the zip code given.\n\n    Args:\n        raw (str): A raw address.\n\n    Returns:\n        dict: A dictionary of address components.\n    \"\"\"\n    if not isinstance(raw, str) or raw == \"\":\n        return {\n            \"raw\": raw,\n            \"address_number\": None,\n            \"street_pre_directional\": None,\n            \"street_name\": None,\n            \"street_post_type\": None,\n            \"unit_type\": None,\n            \"unit_number\": None,\n            \"subaddress_type\": None,\n            \"subaddress_identifier\": None,\n            \"city\": None,\n            \"state\": None,\n            \"postal_code\": None,\n            \"street\": None,\n            \"address_norm\": None,\n        }\n\n    FIELD_NAMES = [\n        \"AddressNumber\",\n        \"StreetNamePreDirectional\",\n        \"StreetName\",\n        \"StreetNamePostType\",\n        \"OccupancyType\",\n        \"OccupancyIdentifier\",\n        \"SubaddressType\",\n        \"SubaddressIdentifier\",\n        \"PlaceName\",\n        \"StateName\",\n        \"ZipCode\",\n    ]\n\n    # remove spaces and punct\n    raw_stripped = re.sub(r\",|\\.\", \"\", raw).strip()\n    # replace # with UNIT\n    to_normalize = re.sub(r\"#\", \" UNIT \", raw_stripped)\n    # replace multiple spaces with single space\n    to_normalize = re.sub(r\"\\s+\", \" \", to_normalize)\n\n    try:\n        normalized = normalize_address_record(to_normalize)\n        normalized = \" \".join(value for value in normalized.values() if value is not None)\n\n    except Exception:\n        normalized = to_normalize\n\n    try:\n        tags = usaddress.tag(normalized)\n        tags = dict(tags[0])\n\n    # retain any successfully parsed fields\n    except usaddress.RepeatedLabelError as e:\n        tags = {}\n\n        for parsed_field in e.parsed_string:\n            value, label = parsed_field\n\n            if label in FIELD_NAMES:\n                tags[label] = value\n\n    record = {\n        \"raw\": raw,\n        \"address_number\": tags.get(\"AddressNumber\"),\n        \"street_pre_directional\": tags.get(\"StreetNamePreDirectional\"),\n        \"street_name\": tags.get(\"StreetName\"),\n        \"street_post_type\": tags.get(\"StreetNamePostType\"),\n        \"unit_type\": tags.get(\"OccupancyType\"),\n        \"unit_number\": tags.get(\"OccupancyIdentifier\"),\n        \"subaddress_type\": tags.get(\"SubaddressType\"),\n        \"subaddress_identifier\": tags.get(\"SubaddressIdentifier\"),\n        \"city\": tags.get(\"PlaceName\"),\n        \"state\": tags.get(\"StateName\"),\n        \"postal_code\": tags.get(\"ZipCode\"),\n        \"address_norm\": str(re.sub(r\"[^a-zA-Z0-9]+\", \"\", raw).upper()),\n    }\n\n    if record[\"city\"] is not None:\n        record[\"city\"] = re.sub(r\"[^A-z\\s]\", \"\", record[\"city\"]).strip()\n\n        if record[\"city\"] == \"\":\n            record[\"city\"] = None\n\n    if record[\"street_name\"] is not None:\n        record[\"street_name\"] = re.sub(r\",\\.\", \"\", record[\"street_name\"]).strip()\n        # Remove unit from street name for cases where the address parser\n        # erroneously included it\n        record[\"street_name\"] = re.sub(r\"UNIT.*\", \"\", record[\"street_name\"]).strip()\n        if record[\"street_name\"] == \"\":\n            record[\"street_name\"] = None\n\n    if record[\"unit_number\"] is not None:\n        record[\"unit_number\"] = re.sub(r\"[^[A-z0-9]\", \"\", record[\"unit_number\"])\n\n        if record[\"unit_number\"] == \"\":\n            record[\"unit_number\"] = None\n\n    # Overwrite city and state using uszip if the parsed state is not valid\n    if record[\"state\"] not in state_abbr or record[\"state\"] is None:\n        zip_city, zip_state = identify_state_city(record[\"postal_code\"])\n\n        # if don't find valid city, state, leave original\n        if zip_city is not None:\n            record[\"city\"] = zip_city\n\n        if zip_state is not None:\n            record[\"state\"] = zip_state\n\n    street_fields = [\n        \"address_number\",\n        \"street_pre_directional\",\n        \"street_name\",\n        \"street_post_type\",\n    ]\n    record[\"street\"] = \" \".join([\n        record[field] for field in street_fields if (record[field] is not None) and (record[field] != \"\")\n    ])\n    if (record[\"street\"] == \"\") or (record[\"street\"] == \" \"):\n        record[\"street\"] = None\n\n    if suffixes.get(record[\"street_post_type\"]):\n        record[\"street_post_type\"] = suffixes.get(record[\"street_post_type\"])\n\n    for key, value in record.items():\n        record[key] = None if value == \"\" else value\n\n    for k, v in record.items():\n        if v is None:\n            continue\n        # Force everything to a Python string:\n        if not isinstance(v, str):\n            record[k] = str(v)\n\n    return record\n</code></pre>"},{"location":"modules/#chainlink.cleaning.cleaning_functions.clean_names","title":"<code>clean_names(raw)</code>","text":"<p>Given a raw name string, clean the name and return it. Contains conditional logic based on the source of the data to handle data-specific cleaning cases. Returns none if the name resembles a list of excluded strings. Strips most non-alphanumeric characters.</p> <p>Parameters:</p> Name Type Description Default <code>raw</code> <code>str</code> <p>A raw name string.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str | None</code> <p>A cleaned name string.</p> Source code in <code>src/chainlink/cleaning/cleaning_functions.py</code> <pre><code>def clean_names(raw: str) -&gt; str | None:\n    \"\"\"\n    Given a raw name string, clean the name and return it. Contains conditional\n    logic based on the source of the data to handle data-specific cleaning cases.\n    Returns none if the name resembles a list of excluded strings. Strips\n    most non-alphanumeric characters.\n\n    Args:\n        raw (str): A raw name string.\n\n    Returns:\n        str: A cleaned name string.\n    \"\"\"\n\n    if re.search(EXCLUDED_PATTERNS, raw):\n        return None\n\n    name = raw.upper()\n\n    name = name.replace(\"&amp;\", \"AND\").replace(\"-\", \" \").replace(\"@\", \"AT\").replace(\"\u2014\", \" \")\n\n    name = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", name)\n    name = re.sub(r\"\\s{2,}\", \" \", name)\n    if (name == \"\") or (name == \" \"):\n        return None\n    else:\n        return name\n    return name\n</code></pre>"},{"location":"modules/#chainlink.cleaning.cleaning_functions.clean_zipcode","title":"<code>clean_zipcode(raw)</code>","text":"<p>Modified from the function written by Anthony Moser of the deseguys project.</p> <p>Returns a 5-digit zipcode from a string.</p> <p>Parameters:</p> Name Type Description Default <code>raw</code> <code>any</code> <p>A zipcode.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A 5-digit zipcode or an empty string.</p> Source code in <code>src/chainlink/cleaning/cleaning_functions.py</code> <pre><code>def clean_zipcode(raw: str | int) -&gt; str:\n    \"\"\"\n    Modified from the function written by Anthony Moser of the deseguys project.\n\n    Returns a 5-digit zipcode from a string.\n\n    Args:\n        raw (any): A zipcode.\n\n    Returns:\n        str: A 5-digit zipcode or an empty string.\n    \"\"\"\n    try:\n        zipcode = str(raw)[:5]\n    except Exception:\n        return \"\"\n    else:\n        return zipcode\n</code></pre>"},{"location":"modules/#chainlink.cleaning.cleaning_functions.identify_state_city","title":"<code>identify_state_city(zipcode)</code>","text":"<p>Use zipcode to look up state and city info using the uszipcode API.</p> <p>Parameters:</p> Name Type Description Default <code>zipcode</code> <code>str</code> <p>A zipcode.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple of city and state, or (None, None) if the lookup failed.</p> Source code in <code>src/chainlink/cleaning/cleaning_functions.py</code> <pre><code>def identify_state_city(zipcode: str) -&gt; tuple:\n    \"\"\"\n    Use zipcode to look up state and city info using the uszipcode API.\n\n    Args:\n        zipcode (str): A zipcode.\n\n    Returns:\n        tuple: A tuple of city and state, or (None, None) if the lookup failed.\n    \"\"\"\n    zipcode = clean_zipcode(zipcode)\n    try:\n        if zipcode in zip_cache:\n            zip_city = zip_cache[zipcode][\"city\"]\n            zip_state = zip_cache[zipcode][\"state\"]\n\n            return (zip_city, zip_state)\n\n        else:\n            engine = SearchEngine()\n            zipcode_obj = engine.by_zipcode(int(zipcode))\n            # zip_cache[zipcode] = zipcode\n\n            zip_city = zipcode_obj.major_city.upper()\n            zip_state = zipcode_obj.state\n\n            zip_citystate = {\"city\": zip_city, \"state\": zip_state}\n            zip_cache[zipcode] = zip_citystate\n\n            return (zip_city, zip_state)\n\n    # Handle cases where zip code is null or not a number\n    except AttributeError:\n        return (None, None)\n\n    except TypeError:\n        return (None, None)\n\n    except ValueError:\n        return (None, None)\n</code></pre>"},{"location":"modules/#chainlink.cleaning.cleaning_functions.predict_org","title":"<code>predict_org(name)</code>","text":"<p>Given a string, predict whether or not the string is an organization name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>An entity name.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>1 if the name is an organization, 0 if the name is an individual.</p> Source code in <code>src/chainlink/cleaning/cleaning_functions.py</code> <pre><code>def predict_org(name: str) -&gt; int:\n    \"\"\"\n    Given a string, predict whether or not the string is an organization name.\n\n    Args:\n        name (str): An entity name.\n\n    Returns:\n        int: 1 if the name is an organization, 0 if the name is an individual.\n    \"\"\"\n    individual_names = re.compile(\n        r\"CURRENT OWNER|TAX PAYER OF|OWNER OF RECORD|PROPERTY OWNER\",\n        flags=re.IGNORECASE,\n    )\n\n    if (\n        re.search(\"0-1\", name)\n        or re.search(ABB_PATTERNS, name)\n        or re.search(WORD_PATTERNS, name)\n        or re.search(EOL_PATTERNS, name)\n    ):\n        return 1\n\n    # Doing this because GX PROPERTY OWNER LLC exists\n    if re.search(individual_names, name):\n        return 0\n\n    else:\n        return 0\n</code></pre>"},{"location":"modules/#chainlink.cleaning.cleaning_functions.remove_initial_I","title":"<code>remove_initial_I(raw)</code>","text":"<p>Remove the \"I\" or I\" present for some names in corporation and LLC data where name was incorrectly entered in the style of \"I, John Smith\" instead of just \"John Smith\"</p> Source code in <code>src/chainlink/cleaning/cleaning_functions.py</code> <pre><code>def remove_initial_I(raw: str) -&gt; str:\n    \"\"\"\n    Remove the \"I\" or I\" present for some names in corporation\n    and LLC data where name was incorrectly entered in the\n    style of \"I, John Smith\" instead of just \"John Smith\"\n    \"\"\"\n    if raw[:3] == '\"I\"':\n        raw = raw[3:]\n    if raw[:2] == 'I\"':\n        raw = raw[2:]\n    return raw\n</code></pre>"},{"location":"modules/#chainlink.link","title":"<code>link</code>","text":""},{"location":"modules/#chainlink.link.link_generic","title":"<code>link_generic</code>","text":""},{"location":"modules/#chainlink.link.link_generic.create_across_links","title":"<code>create_across_links(db_path, new_schema, existing_schema, link_exclusions)</code>","text":"<p>For each entity in the existing_db list, create links between the new entity and the existing entity.</p> for old_entity in existing_db <p>-find all the name links old_entity.name to new_entity.name, etc. -find all the address links old_entity.address to new_entity.address, etc.     -match by raw address string     -match by clean street string     -if street id matches, match by unit     -match street name and number if zipcode matches</p> <p>Returns: None</p> Source code in <code>src/chainlink/link/link_generic.py</code> <pre><code>def create_across_links(db_path: str | Path, new_schema: dict, existing_schema: dict, link_exclusions: list) -&gt; None:\n    \"\"\"\n    For each entity in the existing_db list, create links between the new entity\n    and the existing entity.\n\n    for old_entity in existing_db:\n        -find all the name links old_entity.name to new_entity.name, etc.\n        -find all the address links old_entity.address to new_entity.address, etc.\n            -match by raw address string\n            -match by clean street string\n            -if street id matches, match by unit\n            -match street name and number if zipcode matches\n\n    Returns: None\n    \"\"\"\n\n    new_entity = new_schema[\"schema_name\"]\n\n    new_entity_names = []\n    new_entity_addresses = []\n\n    # gather all the name and address columns for the new entity\n    for table in new_schema[\"tables\"]:\n        for name_col in table[\"name_cols\"]:\n            new_entity_names.append((table[\"table_name\"], table[\"id_col\"], name_col))\n        for address_col in table[\"address_cols\"]:\n            new_entity_addresses.append((table[\"table_name\"], table[\"id_col\"], address_col))\n\n    # create name and address matches for each existing entity and new entity\n    existing_entity = existing_schema[\"schema_name\"]\n\n    existing_entity_names = []\n    existing_entity_addresses = []\n\n    # gather all the name and address columns for this existing entity\n    for table in existing_schema[\"tables\"]:\n        for name_col in table[\"name_cols\"]:\n            existing_entity_names.append((\n                table[\"table_name\"],\n                table[\"id_col\"],\n                name_col,\n            ))\n        for address_col in table[\"address_cols\"]:\n            existing_entity_addresses.append((\n                table[\"table_name\"],\n                table[\"id_col\"],\n                address_col,\n            ))\n    # generate name match combos\n    name_combos = list(itertools.product(new_entity_names, existing_entity_names))\n\n    # need to add in across table within entity combos\n\n    for new, old in name_combos:\n        left_table, left_ent_id, left_name = new\n        right_table, right_ent_id, right_name = old\n\n        execute_match(\n            db_path=db_path,\n            left_entity=new_entity,\n            left_table=left_table,\n            left_matching_col=left_name,\n            left_ent_id=left_ent_id,\n            match_type=\"name_match\",\n            left_matching_id=f\"{left_name}_name_id\",\n            right_entity=existing_entity,\n            right_table=right_table,\n            right_matching_col=right_name,\n            right_ent_id=right_ent_id,\n            right_matching_id=f\"{right_name}_name_id\",\n            link_exclusions=link_exclusions,\n        )\n\n    # generate address match combos\n    address_combos = list(itertools.product(new_entity_addresses, existing_entity_addresses))\n\n    for new, old in address_combos:\n        left_table, left_ent_id, left_address = new\n        right_table, right_ent_id, right_address = old\n\n        execute_match_address(\n            db_path=db_path,\n            left_entity=new_entity,\n            left_table=left_table,\n            left_address=left_address,\n            left_ent_id=left_ent_id,\n            right_entity=existing_entity,\n            right_table=right_table,\n            right_address=right_address,\n            right_ent_id=right_ent_id,\n            skip_address=True,\n            link_exclusions=link_exclusions,\n        )\n</code></pre>"},{"location":"modules/#chainlink.link.link_generic.create_tfidf_across_links","title":"<code>create_tfidf_across_links(db_path, new_schema, existing_schema, link_exclusions)</code>","text":"<p>create all fuzzy links across new entity and existing entity</p> <p>Returns: None</p> Source code in <code>src/chainlink/link/link_generic.py</code> <pre><code>def create_tfidf_across_links(\n    db_path: str | Path, new_schema: dict, existing_schema: dict, link_exclusions: list\n) -&gt; None:\n    \"\"\"\n    create all fuzzy links across new entity and existing entity\n\n    Returns: None\n    \"\"\"\n    new_entity = new_schema[\"schema_name\"]\n\n    # gather all the name columns for the new entity\n    new_entity_names = []\n    new_entity_addresses = []\n\n    for table in new_schema[\"tables\"]:\n        for name_col in table[\"name_cols\"]:\n            new_entity_names.append((table[\"table_name\"], table[\"id_col\"], name_col))\n        for address_col in table[\"address_cols\"]:\n            new_entity_addresses.append((table[\"table_name\"], table[\"id_col\"], address_col))\n\n    # go through all the existing entities / schemas\n\n    existing_entity = existing_schema[\"schema_name\"]\n\n    existing_entity_names = []\n    existing_entity_addresses = []\n\n    # gather all the name and address columns for this existing entity\n    for table in existing_schema[\"tables\"]:\n        for name_col in table[\"name_cols\"]:\n            existing_entity_names.append((\n                table[\"table_name\"],\n                table[\"id_col\"],\n                name_col,\n            ))\n        for address_col in table[\"address_cols\"]:\n            existing_entity_addresses.append((\n                table[\"table_name\"],\n                table[\"id_col\"],\n                address_col,\n            ))\n    # generate name match combos\n\n    name_combos = list(itertools.product(new_entity_names, existing_entity_names))\n\n    for new, old in name_combos:\n        left_table, left_ent_id, left_name = new\n        right_table, right_ent_id, right_name = old\n\n        execute_fuzzy_link(\n            db_path=db_path,\n            left_entity=new_entity,\n            left_table=left_table,\n            left_ent_id=left_ent_id,\n            left_name_col=left_name,\n            right_entity=existing_entity,\n            right_table=right_table,\n            right_ent_id=right_ent_id,\n            right_name_col=right_name,\n            tfidf_table=\"entity.name_similarity\",\n            link_exclusions=link_exclusions,\n        )\n\n    # generate address match combos\n    address_combos = list(itertools.product(new_entity_addresses, existing_entity_addresses))\n    for new, old in address_combos:\n        left_table, left_ent_id, left_address = new\n        right_table, right_ent_id, right_address = old\n\n        execute_address_fuzzy_link(\n            db_path=db_path,\n            left_entity=new_entity,\n            left_table=left_table,\n            left_ent_id=left_ent_id,\n            left_address_col=left_address,\n            right_entity=existing_entity,\n            right_table=right_table,\n            right_ent_id=right_ent_id,\n            right_address_col=right_address,\n            tfidf_table=\"entity.street_name_similarity\",\n            skip_address=True,\n            link_exclusions=link_exclusions,\n        )\n\n    return None\n</code></pre>"},{"location":"modules/#chainlink.link.link_generic.create_tfidf_within_links","title":"<code>create_tfidf_within_links(db_path, schema_config, link_exclusions)</code>","text":"<p>create tfidf links within entity</p> <p>Returns: None</p> Source code in <code>src/chainlink/link/link_generic.py</code> <pre><code>def create_tfidf_within_links(db_path: str | Path, schema_config: dict, link_exclusions: list) -&gt; None:\n    \"\"\"\n    create tfidf links within entity\n\n    Returns: None\n    \"\"\"\n\n    new_entity = schema_config[\"schema_name\"]\n    within_entity_across_tables_names = []\n    within_entity_across_tables_addresses = []\n    # create fuzzy links\n    # generate combos, need all within tables\n\n    for table in schema_config[\"tables\"]:\n        # generate name matches combos\n        name_combos = list(itertools.product(table[\"name_cols\"], repeat=2))\n\n        for left_name, right_name in name_combos:\n            execute_fuzzy_link(\n                db_path=db_path,\n                left_entity=new_entity,\n                left_table=table[\"table_name\"],\n                left_ent_id=table[\"id_col\"],\n                left_name_col=left_name,\n                right_entity=new_entity,\n                right_table=table[\"table_name\"],\n                right_ent_id=table[\"id_col\"],\n                right_name_col=right_name,\n                tfidf_table=\"entity.name_similarity\",\n                link_exclusions=link_exclusions,\n            )\n\n        address_combos = list(itertools.product(table[\"address_cols\"], repeat=2))\n        for left_address, right_address in address_combos:\n            execute_address_fuzzy_link(\n                db_path=db_path,\n                left_entity=new_entity,\n                left_table=table[\"table_name\"],\n                left_ent_id=table[\"id_col\"],\n                left_address_col=left_address,\n                right_entity=new_entity,\n                right_table=table[\"table_name\"],\n                right_ent_id=table[\"id_col\"],\n                right_address_col=right_address,\n                tfidf_table=\"entity.street_name_similarity\",\n                skip_address=True,\n                link_exclusions=link_exclusions,\n            )\n\n        # for across tables within entity\n        within_entity_across_tables_names.append([\n            (name, table[\"table_name\"], table[\"id_col\"]) for name in table[\"name_cols\"]\n        ])\n        within_entity_across_tables_addresses.append([\n            (address, table[\"table_name\"], table[\"id_col\"]) for address in table[\"address_cols\"]\n        ])\n\n    # generate combos, across tables within entity\n    across_name_combos, across_address_combos = generate_combos_within_across_tables(\n        within_entity_across_tables_names, within_entity_across_tables_addresses\n    )\n\n    for left, right in across_name_combos:\n        left_name, left_table, left_ent_id = left\n        right_name, right_table, right_ent_id = right\n\n        execute_fuzzy_link(\n            db_path=db_path,\n            left_entity=new_entity,\n            left_table=left_table,\n            left_ent_id=left_ent_id,\n            left_name_col=left_name,\n            right_entity=new_entity,\n            right_table=right_table,\n            right_ent_id=right_ent_id,\n            right_name_col=right_name,\n            tfidf_table=\"entity.name_similarity\",\n            link_exclusions=link_exclusions,\n        )\n\n    for left, right in across_address_combos:\n        left_address, left_table, left_ent_id = left\n        right_address, right_table, right_ent_id = right\n        execute_address_fuzzy_link(\n            db_path=db_path,\n            left_entity=new_entity,\n            left_table=left_table,\n            left_ent_id=left_ent_id,\n            left_address_col=left_address,\n            right_entity=new_entity,\n            right_table=right_table,\n            right_ent_id=right_ent_id,\n            right_address_col=right_address,\n            tfidf_table=\"entity.street_name_similarity\",\n            skip_address=True,\n            link_exclusions=link_exclusions,\n        )\n</code></pre>"},{"location":"modules/#chainlink.link.link_generic.create_within_links","title":"<code>create_within_links(db_path, schema_config, link_exclusions)</code>","text":"<p>Creates exact string matches on name and address fields for entity and entity.</p> For each file find the links with the file <p>-find all the name links includes name1 to name1, name1 to name2, etc. -find all the address links includes address1 to address1, address1 to address2, etc.     -match by raw address string     -match by clean street string     -if street id matches, match by unit     -match street name and number if zipcode matches -find all name and address links across tables within the entity</p> <p>Returns: None</p> Source code in <code>src/chainlink/link/link_generic.py</code> <pre><code>def create_within_links(db_path: str | Path, schema_config: dict, link_exclusions: list) -&gt; None:\n    \"\"\"\n    Creates exact string matches on name and address fields for entity and\n    entity.\n\n    For each file find the links with the file:\n        -find all the name links includes name1 to name1, name1 to name2, etc.\n        -find all the address links includes address1 to address1, address1 to address2, etc.\n            -match by raw address string\n            -match by clean street string\n            -if street id matches, match by unit\n            -match street name and number if zipcode matches\n        -find all name and address links across tables within the entity\n\n    Returns: None\n    \"\"\"\n\n    entity = schema_config[\"schema_name\"]\n\n    within_entity_across_tables_names = []\n    within_entity_across_tables_addresses = []\n\n    # within each table\n    for table_config in schema_config[\"tables\"]:\n        table = table_config[\"table_name\"]\n\n        if table_config.get(\"name_cols\"):\n            # generate name matches combos\n            name_combos = list(itertools.product(table_config[\"name_cols\"], repeat=2))\n\n            for left_name, right_name in name_combos:\n                execute_match(\n                    db_path=db_path,\n                    match_type=\"name_match\",\n                    left_entity=entity,\n                    left_table=table,\n                    left_matching_col=left_name,\n                    left_matching_id=f\"{left_name}_name_id\",\n                    left_ent_id=table_config[\"id_col\"],\n                    right_entity=entity,\n                    right_table=table,\n                    right_matching_col=right_name,\n                    right_ent_id=table_config[\"id_col\"],\n                    right_matching_id=f\"{right_name}_name_id\",\n                    link_exclusions=link_exclusions,\n                )\n\n        if table_config.get(\"address_cols\"):\n            # address within\n            address_combos = list(itertools.product(table_config[\"address_cols\"], repeat=2))\n\n            for left_address, right_address in address_combos:\n                execute_match_address(\n                    db_path=db_path,\n                    left_entity=entity,\n                    left_table=table,\n                    left_address=left_address,\n                    left_ent_id=table_config[\"id_col\"],\n                    right_entity=entity,\n                    right_table=table,\n                    right_address=right_address,\n                    right_ent_id=table_config[\"id_col\"],\n                    skip_address=True,\n                    link_exclusions=link_exclusions,\n                )\n\n        # for across tables\n        if table_config.get(\"name_cols\"):\n            within_entity_across_tables_names.append([\n                (name, table, table_config[\"id_col\"]) for name in table_config[\"name_cols\"]\n            ])\n        if table_config.get(\"address_cols\"):\n            within_entity_across_tables_addresses.append([\n                (address, table, table_config[\"id_col\"]) for address in table_config[\"address_cols\"]\n            ])\n\n    # generate combos across tables\n    if within_entity_across_tables_names or within_entity_across_tables_addresses:\n        across_name_combos, across_address_combos = generate_combos_within_across_tables(\n            within_entity_across_tables_names, within_entity_across_tables_addresses\n        )\n\n    # across files for name\n    for left, right in across_name_combos:\n        left_name, left_table, left_ent_id = left\n        right_name, right_table, right_ent_id = right\n\n        execute_match(\n            db_path=db_path,\n            match_type=\"name_match\",\n            left_entity=entity,\n            left_table=left_table,\n            left_matching_col=left_name,\n            left_ent_id=left_ent_id,\n            left_matching_id=f\"{left_name}_name_id\",\n            right_entity=entity,\n            right_table=right_table,\n            right_matching_col=right_name,\n            right_ent_id=right_ent_id,\n            right_matching_id=f\"{right_name}_name_id\",\n            link_exclusions=link_exclusions,\n        )\n\n    # across files for address\n    for left, right in across_address_combos:\n        left_address, left_table, left_ent_id = left\n        right_address, right_table, right_ent_id = right\n\n        execute_match_address(\n            db_path=db_path,\n            left_entity=entity,\n            left_table=left_table,\n            left_address=left_address,\n            left_ent_id=left_ent_id,\n            right_entity=entity,\n            right_table=right_table,\n            right_address=right_address,\n            right_ent_id=right_ent_id,\n            skip_address=True,\n            link_exclusions=link_exclusions,\n        )\n</code></pre>"},{"location":"modules/#chainlink.link.link_utils","title":"<code>link_utils</code>","text":""},{"location":"modules/#chainlink.link.link_utils.execute_address_fuzzy_link","title":"<code>execute_address_fuzzy_link(db_path, left_entity, left_table, left_ent_id, left_address_col, right_entity, right_table, right_ent_id, right_address_col, tfidf_table='link.tfidf_staging', skip_address=False, link_exclusions=None)</code>","text":"<p>fuzzy address matching</p> Source code in <code>src/chainlink/link/link_utils.py</code> <pre><code>def execute_address_fuzzy_link(\n    db_path: str | Path,\n    left_entity: str,\n    left_table: str,\n    left_ent_id: str,\n    left_address_col: str,\n    right_entity: str,\n    right_table: str,\n    right_ent_id: str,\n    right_address_col: str,\n    tfidf_table: str = \"link.tfidf_staging\",\n    skip_address: bool = False,\n    link_exclusions: Optional[list] = None,\n) -&gt; None:\n    \"\"\"\n    fuzzy address matching\n    \"\"\"\n    if link_exclusions is None:\n        link_exclusions = []\n\n    link_table = f\"link.{left_entity}_{right_entity}\"\n\n    # align the names of the match columns\n    left_side = f\"{left_entity}_{left_table}_{left_address_col}\"\n    right_side = f\"{right_entity}_{right_table}_{right_address_col}\"\n    if left_entity != right_entity:\n        match_name_stem = f\"{left_side}_{right_side}\" if left_side &lt; right_side else f\"{right_side}_{left_side}\"\n    else:\n        match_name_stem = f\"{left_side}_{right_side}\"\n\n    same_condition = \"TRUE\"\n\n    if left_ent_id == right_ent_id and left_entity == right_entity:\n        left_ent_id_rename = f\"{left_ent_id}_1\"\n        right_ent_id_rename = f\"{right_ent_id}_2\"\n        left_unit_num_rename = f\"{left_address_col}_unit_number_1\"\n        right_unit_num_rename = f\"{right_address_col}_unit_number_2\"\n        left_address_num_rename = f\"{left_address_col}_address_number_1\"\n        right_address_num_rename = f\"{right_address_col}_address_number_2\"\n        left_postal_code_rename = f\"{left_address_col}_postal_code_1\"\n        right_postal_code_rename = f\"{right_address_col}_postal_code_2\"\n        left_directional_rename = f\"{left_address_col}_street_pre_directional_1\"\n        right_directional_rename = f\"{right_address_col}_street_pre_directional_2\"\n        # if same id, want to remove dupes\n        same_condition = f\"{left_entity}_{left_ent_id_rename} &lt; {right_entity}_{right_ent_id_rename}\"\n    else:\n        left_ent_id_rename = left_ent_id\n        right_ent_id_rename = right_ent_id\n        left_unit_num_rename = f\"{left_address_col}_unit_number\"\n        right_unit_num_rename = f\"{right_address_col}_unit_number\"\n        left_address_num_rename = f\"{left_address_col}_address_number\"\n        right_address_num_rename = f\"{right_address_col}_address_number\"\n        left_postal_code_rename = f\"{left_address_col}_postal_code\"\n        right_postal_code_rename = f\"{right_address_col}_postal_code\"\n        left_directional_rename = f\"{left_address_col}_street_pre_directional\"\n        right_directional_rename = f\"{right_address_col}_street_pre_directional\"\n\n    if skip_address:\n        address_condition = \" != 1\"\n        left_address_condition = f\"{left_address_col}_skip {address_condition}\"\n        right_address_condition = f\"{right_address_col}_skip {address_condition}\"\n    else:\n        left_address_condition = \"TRUE\"\n        right_address_condition = \"TRUE\"\n\n    match_names = [\n        f\"{match_name_stem}_street_fuzzy_match\",\n        f\"{match_name_stem}_unit_fuzzy_match\",\n    ]\n\n    conditions = [\n        f\"\"\"\n        {left_entity}_{left_address_num_rename} = {right_entity}_{right_address_num_rename} AND\n        {left_entity}_{left_postal_code_rename} = {right_entity}_{right_postal_code_rename}\"\"\",\n        f\"\"\"\n        {left_entity}_{left_address_num_rename} = {right_entity}_{right_address_num_rename} AND\n        {left_entity}_{left_postal_code_rename} = {right_entity}_{right_postal_code_rename} AND\n        CAST({left_entity}_{left_unit_num_rename} AS VARCHAR) = CAST({right_entity}_{right_unit_num_rename} AS VARCHAR)\n        \"\"\",\n    ]\n    for match_name, condition in zip(match_names, conditions):\n        # check link exclusion\n        if any(exclusion in match_name for exclusion in link_exclusions):\n            return None\n        query = f\"\"\"\n        CREATE OR REPLACE TABLE {link_table} AS\n\n        WITH tfidf_matches AS (\n            SELECT id_a,\n                id_b,\n                similarity as {match_name}\n            FROM {tfidf_table}\n        ),\n\n        left_source AS (\n            SELECT {left_ent_id} as {left_entity}_{left_ent_id_rename},\n                    {left_address_col}_street_name_id,\n                    {left_address_col}_unit_number as {left_entity}_{left_unit_num_rename},\n                    {left_address_col}_street_pre_directional as {left_entity}_{left_directional_rename},\n                    {left_address_col}_address_number as {left_entity}_{left_address_num_rename},\n                    {left_address_col}_postal_code as {left_entity}_{left_postal_code_rename}\n            FROM {left_entity}.{left_table}\n            WHERE {left_address_condition}\n        ),\n\n        right_source AS (\n            SELECT {right_ent_id} as {right_entity}_{right_ent_id_rename},\n                    {right_address_col}_street_name_id,\n                    {right_address_col}_unit_number as {right_entity}_{right_unit_num_rename},\n                    {right_address_col}_street_pre_directional as {right_entity}_{right_directional_rename},\n                    {right_address_col}_address_number as {right_entity}_{right_address_num_rename},\n                    {right_address_col}_postal_code as {right_entity}_{right_postal_code_rename}\n            FROM {right_entity}.{right_table}\n            WHERE {right_address_condition}\n        ),\n\n        fuzzy_match_1 AS (\n            SELECT l.{left_entity}_{left_ent_id_rename},\n                l.{left_entity}_{left_unit_num_rename}, l.{left_entity}_{left_address_num_rename},\n                l.{left_entity}_{left_postal_code_rename}, l.{left_entity}_{left_directional_rename},\n                r.{right_entity}_{right_ent_id_rename},\n                r.{right_entity}_{right_unit_num_rename}, r.{right_entity}_{right_address_num_rename},\n                r.{right_entity}_{right_postal_code_rename}, r.{right_entity}_{right_directional_rename},\n                m.{match_name}\n            FROM   tfidf_matches as m\n            INNER JOIN left_source as l\n                ON m.id_a = l.{left_address_col}_street_name_id\n            INNER JOIN right_source as r\n                ON m.id_b = r.{right_address_col}_street_name_id\n        ),\n\n        fuzzy_match_2 AS (\n            SELECT l.{left_entity}_{left_ent_id_rename},\n                l.{left_entity}_{left_unit_num_rename}, l.{left_entity}_{left_address_num_rename},\n                l.{left_entity}_{left_postal_code_rename}, l.{left_entity}_{left_directional_rename},\n                r.{right_entity}_{right_ent_id_rename},\n                r.{right_entity}_{right_unit_num_rename}, r.{right_entity}_{right_address_num_rename},\n                r.{right_entity}_{right_postal_code_rename}, r.{right_entity}_{right_directional_rename},\n                m.{match_name}\n            FROM   tfidf_matches as m\n            INNER JOIN left_source as l\n                ON m.id_b = l.{left_address_col}_street_name_id\n            INNER JOIN right_source as r\n                ON m.id_a = r.{right_address_col}_street_name_id\n        ),\n\n        all_fuzzy_matches AS (\n            SELECT {left_entity}_{left_ent_id_rename},\n                    {right_entity}_{right_ent_id_rename},\n                    {match_name}\n            FROM (SELECT * FROM fuzzy_match_1\n                UNION\n                SELECT * FROM fuzzy_match_2)\n            WHERE {same_condition} AND\n            {condition}\n\n        ),\n\n        existing_links AS (\n            SELECT *\n            FROM {link_table}\n        )\n\n        SELECT *\n        FROM   all_fuzzy_matches\n        FULL JOIN existing_links\n            USING({left_entity}_{left_ent_id_rename},{right_entity}_{right_ent_id_rename})\n\n        \"\"\"\n\n        with duckdb.connect(database=db_path, read_only=False) as db_conn:\n            db_conn.execute(query)\n            console.log(f\"[yellow] Created {match_name}\")\n            logger.debug(f\"Created {match_name}\")\n            cols = [row[1] for row in db_conn.execute(f\"PRAGMA table_info('{link_table}')\").fetchall()]\n            for col in cols:\n                db_conn.execute(f\"UPDATE {link_table} SET {col} = 0 WHERE {col} IS NULL\")\n\n            # set datatype to int or float as expected\n            if \"fuzzy\" in match_name:\n                db_conn.execute(f\"UPDATE {link_table} SET {match_name} = CAST({match_name} AS FLOAT)\")\n            else:\n                db_conn.execute(f\"UPDATE {link_table} SET {match_name} = CAST({match_name} AS INT1)\")\n\n    return None\n</code></pre>"},{"location":"modules/#chainlink.link.link_utils.execute_fuzzy_link","title":"<code>execute_fuzzy_link(db_path, left_entity, left_table, left_ent_id, left_name_col, right_entity, right_table, right_ent_id, right_name_col, tfidf_table='link.tfidf_staging', link_exclusions=None)</code>","text":"<p>Given two tables and a tfidf matching entity table, create a fuzzy match between the two tables. Creates a match column called {left_entity}{left_table}{left_name_col}{right_entity}{right_table}{right_name_col}_fuzzy_match and appends to link table link.{left_entity}{right_entity}</p> Source code in <code>src/chainlink/link/link_utils.py</code> <pre><code>def execute_fuzzy_link(\n    db_path: str | Path,\n    left_entity: str,\n    left_table: str,\n    left_ent_id: str,\n    left_name_col: str,\n    right_entity: str,\n    right_table: str,\n    right_ent_id: str,\n    right_name_col: str,\n    tfidf_table: str = \"link.tfidf_staging\",\n    link_exclusions: Optional[list] = None,\n) -&gt; None:\n    \"\"\"\n\n    Given two tables and a tfidf matching entity table, create a fuzzy match between the two tables.\n    Creates a match column called\n    {left_entity}_{left_table}_{left_name_col}_{right_entity}_{right_table}_{right_name_col}_fuzzy_match\n    and appends to link table link.{left_entity}_{right_entity}\n    \"\"\"\n    if link_exclusions is None:\n        link_exclusions = []\n\n    link_table = f\"link.{left_entity}_{right_entity}\"\n\n    # align the names of the match columns\n    left_side = f\"{left_entity}_{left_table}_{left_name_col}\"\n    right_side = f\"{right_entity}_{right_table}_{right_name_col}\"\n    if left_entity != right_entity:\n        if left_side &lt; right_side:\n            match_name = f\"{left_side}_{right_side}_fuzzy_match\"\n        else:\n            match_name = f\"{right_side}_{left_side}_fuzzy_match\"\n    else:\n        match_name = f\"{left_side}_{right_side}_fuzzy_match\"\n\n    # check link exclusion\n    if any(exclusion in match_name for exclusion in link_exclusions):\n        return None\n\n    same_condition = \"TRUE\"\n\n    if left_ent_id == right_ent_id and left_entity == right_entity:\n        left_ent_id_rename = f\"{left_ent_id}_1\"\n        right_ent_id_rename = f\"{right_ent_id}_2\"\n        # if same id, want to remove dupes\n        same_condition = f\"{left_entity}_{left_ent_id_rename} &lt; {right_entity}_{right_ent_id_rename}\"\n    else:\n        left_ent_id_rename = left_ent_id\n        right_ent_id_rename = right_ent_id\n\n    query = f\"\"\"\n    CREATE OR REPLACE TABLE {link_table} AS\n\n    WITH tfidf_matches AS (\n        SELECT id_a,\n               id_b,\n               similarity as {match_name}\n        FROM {tfidf_table}\n    ),\n\n    left_source AS (\n        SELECT {left_ent_id} as {left_entity}_{left_ent_id_rename},\n                {left_name_col}_name_id\n        FROM {left_entity}.{left_table}\n    ),\n\n    right_source AS (\n        SELECT {right_ent_id} as {right_entity}_{right_ent_id_rename},\n               {right_name_col}_name_id\n        FROM {right_entity}.{right_table}\n    ),\n\n    fuzzy_match_1 AS (\n        SELECT l.{left_entity}_{left_ent_id_rename},\n               r.{right_entity}_{right_ent_id_rename},\n               m.{match_name}\n        FROM   tfidf_matches as m\n        INNER JOIN left_source as l\n            ON m.id_a = l.{left_name_col}_name_id\n        INNER JOIN right_source as r\n            ON m.id_b = r.{right_name_col}_name_id\n    ),\n\n    fuzzy_match_2 AS (\n        SELECT l.{left_entity}_{left_ent_id_rename},\n               r.{right_entity}_{right_ent_id_rename},\n               m.{match_name}\n        FROM   tfidf_matches as m\n        INNER JOIN left_source as l\n            ON m.id_b = l.{left_name_col}_name_id\n        INNER JOIN right_source as r\n            ON m.id_a = r.{right_name_col}_name_id\n    ),\n\n    all_fuzzy_matches AS (\n        SELECT *\n        FROM (SELECT * FROM fuzzy_match_1\n              UNION\n              SELECT * FROM fuzzy_match_2)\n        where {same_condition}\n    ),\n\n    existing_links AS (\n        SELECT *\n        FROM {link_table}\n    )\n\n    SELECT *\n    FROM   all_fuzzy_matches\n    FULL JOIN existing_links\n        USING({left_entity}_{left_ent_id_rename},{right_entity}_{right_ent_id_rename})\n\n    \"\"\"\n\n    with duckdb.connect(database=db_path, read_only=False) as db_conn:\n        db_conn.execute(query)\n        console.log(f\"[yellow] Created {match_name}\")\n        logger.debug(f\"Created {match_name}\")\n        cols = [row[1] for row in db_conn.execute(f\"PRAGMA table_info('{link_table}')\").fetchall()]\n        for col in cols:\n            db_conn.execute(f\"UPDATE {link_table} SET {col} = 0 WHERE {col} IS NULL\")\n\n        # set datatype to int or float as expected\n        if \"fuzzy\" in match_name:\n            db_conn.execute(f\"UPDATE {link_table} SET {match_name} = CAST({match_name} AS FLOAT)\")\n        else:\n            db_conn.execute(f\"UPDATE {link_table} SET {match_name} = CAST({match_name} AS INT1)\")\n\n    return None\n</code></pre>"},{"location":"modules/#chainlink.link.link_utils.execute_match","title":"<code>execute_match(db_path, match_type, left_entity, left_table, left_matching_col, left_matching_id, left_ent_id, right_entity, right_table, right_matching_col, right_matching_id, right_ent_id, skip_address=False, link_exclusions=None)</code>","text":"<p>Exact matches between two column in two tables. Creates a match column called {left_entity}{left_table}{left_matching_col}{right_entity}{right_table}{right_matching_col}{match_type} and appends to link table link.{left_entity}_{right_entity}</p> <p>Returns: None</p> Source code in <code>src/chainlink/link/link_utils.py</code> <pre><code>def execute_match(\n    db_path: str | Path,\n    match_type: str,\n    left_entity: str,\n    left_table: str,\n    left_matching_col: str,\n    left_matching_id: str,\n    left_ent_id: str,\n    right_entity: str,\n    right_table: str,\n    right_matching_col: str,\n    right_matching_id: str,\n    right_ent_id: str,\n    skip_address: bool = False,\n    link_exclusions: Optional[list] = None,\n) -&gt; None:\n    \"\"\"\n    Exact matches between two column in two tables.\n    Creates a match column called\n    {left_entity}_{left_table}_{left_matching_col}_{right_entity}_{right_table}_{right_matching_col}_{match_type}\n    and appends to link table link.{left_entity}_{right_entity}\n\n    Returns: None\n    \"\"\"\n    if link_exclusions is None:\n        link_exclusions = []\n\n    # if two different ids just dont want duplicates\n    matching_condition = \"!=\"\n\n    if left_ent_id == right_ent_id and left_entity == right_entity:\n        left_ent_id_edit = f\"{left_ent_id}_1\"\n        right_ent_id_edit = f\"{right_ent_id}_2\"\n        # if same id, only want one direction of matches\n        matching_condition = \"&lt;\"\n    else:\n        left_ent_id_edit = left_ent_id\n        right_ent_id_edit = right_ent_id\n\n    link_table = f\"link.{left_entity}_{right_entity}\"\n\n    # align the names of the match columns\n    left_side = f\"{left_entity}_{left_table}_{left_matching_col}\"\n    right_side = f\"{right_entity}_{right_table}_{right_matching_col}\"\n    if left_entity != right_entity:\n        if left_side &lt; right_side:\n            match_name_col = f\"{left_side}_{right_side}_{match_type}\"\n        else:\n            match_name_col = f\"{right_side}_{left_side}_{match_type}\"\n    else:\n        match_name_col = f\"{left_side}_{right_side}_{match_type}\"\n\n    # check link exclusion\n    if any(exclusion in match_name_col for exclusion in link_exclusions):\n        return None\n\n    if skip_address:\n        address_condition = \" != 1\"\n        left_address_condition = f\"l.{left_matching_col}_skip {address_condition}\"\n        right_address_condition = f\"r.{right_matching_col}_skip {address_condition}\"\n        left_extra_col = f\", {left_matching_col}_skip\"\n        right_extra_col = f\", {right_matching_col}_skip\"\n    else:\n        left_address_condition = \"TRUE\"\n        right_address_condition = \"TRUE\"\n        left_extra_col = \"\"\n        right_extra_col = \"\"\n\n    temp_table = match_name_col + \"_table\"\n\n    matching_query = f\"\"\"\n            CREATE SCHEMA IF NOT EXISTS link;\n\n            CREATE OR REPLACE TABLE link.{temp_table} AS\n            SELECT l.{left_entity}_{left_ent_id_edit},\n                   r.{right_entity}_{right_ent_id_edit},\n                   1 AS {match_name_col}\n            FROM\n                (SELECT {left_ent_id} AS {left_entity}_{left_ent_id_edit},\n                        {left_matching_id} {left_extra_col}\n                FROM {left_entity}.{left_table}\n                ) as l\n            JOIN\n                (SELECT {right_ent_id} AS {right_entity}_{right_ent_id_edit},\n                        {right_matching_id} {right_extra_col}\n                FROM {right_entity}.{right_table}\n                ) as r\n                ON l.{left_matching_id} = r.{right_matching_id}\n                AND l.{left_matching_id} IS NOT NULL\n                AND r.{right_matching_id} IS NOT NULL\n                AND l.{left_entity}_{left_ent_id_edit} {matching_condition} r.{right_entity}_{right_ent_id_edit}\n            WHERE\n                {left_address_condition}\n                AND {right_address_condition}\n        ;\"\"\"\n\n    with duckdb.connect(database=db_path, read_only=False) as db_conn:\n        db_conn.execute(matching_query)\n        console.log(f\"[yellow] Created {match_name_col}\")\n        logger.debug(f\"Created {match_name_col}\")\n\n        execute_match_processing(\n            db_conn=db_conn,\n            link_table=link_table,\n            out_temp_table_name=temp_table,\n            id_col_1=f\"{left_entity}_{left_ent_id_edit}\",\n            match_name_col=match_name_col,\n            id_col_2=f\"{right_entity}_{right_ent_id_edit}\",\n        )\n        logger.debug(f\"Finished match processing for {match_name_col}\")\n\n    return None\n</code></pre>"},{"location":"modules/#chainlink.link.link_utils.execute_match_address","title":"<code>execute_match_address(db_path, left_entity, left_table, left_address, left_ent_id, right_entity, right_table, right_address, right_ent_id, skip_address=False, link_exclusions=None)</code>","text":"<p>given a two address columns, match the addresses:     * match by raw address string     * match by clean street string     * if street id matches, match by unit     * match street name and number if zipcode matches</p> <p>Creates four match columns called {left_entity}{left_table}{left_matching_col}{right_entity}{right_table}{right_matching_col}{match_type} and appends to link table link.{left_entity}_{right_entity}</p> <p>Returns: None</p> Source code in <code>src/chainlink/link/link_utils.py</code> <pre><code>def execute_match_address(\n    db_path: str | Path,\n    left_entity: str,\n    left_table: str,\n    left_address: str,\n    left_ent_id: str,\n    right_entity: str,\n    right_table: str,\n    right_address: str,\n    right_ent_id: str,\n    skip_address: bool = False,\n    link_exclusions: Optional[list] = None,\n) -&gt; None:\n    \"\"\"\n    given a two address columns, match the addresses:\n        * match by raw address string\n        * match by clean street string\n        * if street id matches, match by unit\n        * match street name and number if zipcode matches\n\n    Creates four match columns called\n    {left_entity}_{left_table}_{left_matching_col}_{right_entity}_{right_table}_{right_matching_col}_{match_type}\n    and appends to link table link.{left_entity}_{right_entity}\n\n\n\n    Returns: None\n    \"\"\"\n    if link_exclusions is None:\n        link_exclusions = []\n\n    ## Match by raw address string and by street id\n    for match in [\"street\", \"address\"]:\n        logger.debug(f\"Executing {match} match\")\n        execute_match(\n            db_path=db_path,\n            match_type=f\"{match}_match\",\n            left_entity=left_entity,\n            left_table=left_table,\n            left_matching_col=left_address,\n            left_matching_id=f\"{left_address}_{match}_id\",\n            left_ent_id=left_ent_id,\n            right_entity=right_entity,\n            right_table=right_table,\n            right_matching_col=right_address,\n            right_matching_id=f\"{right_address}_{match}_id\",\n            right_ent_id=right_ent_id,\n            skip_address=skip_address,\n            link_exclusions=link_exclusions,\n        )\n\n    ## If street id matches, match by unit\n    left_side = f\"{left_entity}_{left_table}_{left_address}\"\n    right_side = f\"{right_entity}_{right_table}_{right_address}\"\n    if left_entity != right_entity:\n        if left_side &lt; right_side:\n            street_match_to_check = f\"{left_side}_{right_side}_street_match\"\n        else:\n            street_match_to_check = f\"{right_side}_{left_side}_street_match\"\n    else:\n        street_match_to_check = f\"{left_side}_{right_side}_street_match\"\n\n    execute_match_unit(\n        db_path=db_path,\n        left_entity=left_entity,\n        right_entity=right_entity,\n        # TODO will ording of left and right address mess things up\n        street_match_to_check=street_match_to_check,\n        left_table=left_table,\n        left_address=left_address,\n        left_ent_id=left_ent_id,\n        right_table=right_table,\n        right_address=right_address,\n        right_ent_id=right_ent_id,\n        skip_address=skip_address,\n        link_exclusions=link_exclusions,\n    )\n</code></pre>"},{"location":"modules/#chainlink.link.link_utils.execute_match_processing","title":"<code>execute_match_processing(db_conn, link_table, out_temp_table_name, id_col_1, match_name_col, id_col_2)</code>","text":"<p>Steps to run after matches are created. append matches to link table, set null matches to 0, and drop temp table of matches runs in execute_match()</p> <p>Returns: None</p> Source code in <code>src/chainlink/link/link_utils.py</code> <pre><code>def execute_match_processing(\n    db_conn: DuckDBPyConnection,\n    link_table: str,\n    out_temp_table_name: str,\n    id_col_1: str,\n    match_name_col: str,\n    id_col_2: str,\n) -&gt; None:\n    \"\"\"\n    Steps to run after matches are created.\n    append matches to link table, set null matches to 0, and drop temp table of matches\n    runs in execute_match()\n\n    Returns: None\n    \"\"\"\n    # check if link table exists\n    link_table_check = db_conn.execute(\n        f\"\"\"SELECT COUNT(*)\n        FROM information_schema.tables\n        WHERE table_name = '{link_table.split(\".\")[1]}'\n               and table_schema = '{link_table.split(\".\")[0]}'\"\"\"\n    ).fetchone()[0]\n\n    link_table_exists = link_table_check != 0\n\n    # append to link table\n    db_conn.execute(query_append_to_links(link_table_exists, link_table, out_temp_table_name, id_col_1, id_col_2))\n\n    # set null matches to 0\n    db_conn.execute(f\"UPDATE {link_table} SET {match_name_col} = 0 WHERE {match_name_col} IS NULL\")\n\n    cols = [row[1] for row in db_conn.execute(f\"PRAGMA table_info('{link_table}')\").fetchall()]\n    for col in cols:\n        db_conn.execute(f\"UPDATE {link_table} SET {col} = 0 WHERE {col} IS NULL\")\n\n    # set datatype to int or float as expected\n    if \"fuzzy\" in match_name_col:\n        db_conn.execute(f\"UPDATE {link_table} SET {match_name_col} = CAST({match_name_col} AS FLOAT)\")\n    else:\n        db_conn.execute(f\"UPDATE {link_table} SET {match_name_col} = CAST({match_name_col} AS INT1)\")\n\n    # drop temp table of matches\n    db_conn.execute(f\"DROP TABLE link.{out_temp_table_name}\")\n</code></pre>"},{"location":"modules/#chainlink.link.link_utils.execute_match_unit","title":"<code>execute_match_unit(db_path, left_entity, right_entity, street_match_to_check, left_table, left_address, left_ent_id, right_table, right_address, right_ent_id, skip_address=False, link_exclusions=None)</code>","text":"<p>Given two address columns, if street id matches, match by unit.</p> <p>Creates a match column called {left_entity}{left_table}{left_address}{right_entity}{right_table}{right_address}_unit_match and appends to link table link.{left_entity}{right_entity}</p> Source code in <code>src/chainlink/link/link_utils.py</code> <pre><code>def execute_match_unit(\n    db_path: str | Path,\n    left_entity: str,\n    right_entity: str,\n    street_match_to_check: str,\n    left_table: str,\n    left_address: str,\n    left_ent_id: str,\n    right_table: str,\n    right_address: str,\n    right_ent_id: str,\n    skip_address: bool = False,\n    link_exclusions: Optional[list] = None,\n) -&gt; None:\n    \"\"\"\n    Given two address columns, if street id matches, match by unit.\n\n    Creates a match column called\n    {left_entity}_{left_table}_{left_address}_{right_entity}_{right_table}_{right_address}_unit_match\n    and appends to link table link.{left_entity}_{right_entity}\n    \"\"\"\n    if link_exclusions is None:\n        link_exclusions = []\n\n    # if same id, only want one direction of matches\n    if left_ent_id == right_ent_id and left_entity == right_entity:\n        left_ent_id_edit = f\"{left_ent_id}_1\"\n        right_ent_id_edit = f\"{right_ent_id}_2\"\n    else:\n        left_ent_id_edit = left_ent_id\n        right_ent_id_edit = right_ent_id\n\n    link_table = f\"link.{left_entity}_{right_entity}\"\n\n    # align the names of the match columns\n    left_side = f\"{left_entity}_{left_table}_{left_address}\"\n    right_side = f\"{right_entity}_{right_table}_{right_address}\"\n    if left_entity != right_entity:\n        if left_side &lt; right_side:\n            match_name_col = f\"{left_side}_{right_side}_unit_match\"\n        else:\n            match_name_col = f\"{right_side}_{left_side}_unit_match\"\n    else:\n        match_name_col = f\"{left_side}_{right_side}_unit_match\"\n    # check link exclusion\n    if any(exclusion in match_name_col for exclusion in link_exclusions):\n        return None\n\n    if skip_address:\n        address_condition = \" != 1\"\n        left_address_condition = f\"{left_address}_skip {address_condition}\"\n        right_address_condition = f\"{right_address}_skip {address_condition}\"\n    else:\n        left_address_condition = \"TRUE\"\n        right_address_condition = \"TRUE\"\n\n    temp_table = match_name_col + \"_table\"\n\n    matching_query = f\"\"\"\n\n        CREATE OR REPLACE TABLE link.{temp_table} AS\n\n        WITH link as (\n            SELECT {left_entity}_{left_ent_id_edit},\n                    {right_entity}_{right_ent_id_edit}\n            FROM {link_table}\n            WHERE {street_match_to_check} = 1\n            )\n\n        ,lhs as (\n            SELECT {left_ent_id} AS {left_entity}_{left_ent_id_edit},\n                    {left_address}_unit_number AS unit_1\n            FROM {left_entity}.{left_table}\n            where {left_address_condition}\n\n            )\n\n        , rhs as (\n            SELECT {right_ent_id} AS {right_entity}_{right_ent_id_edit},\n                    {right_address}_unit_number AS unit_2\n            FROM {right_entity}.{right_table}\n            where {right_address_condition}\n            )\n\n        SELECT {left_entity}_{left_ent_id_edit},\n               {right_entity}_{right_ent_id_edit},\n               1 AS {match_name_col}\n        FROM link\n        LEFT JOIN lhs\n        USING({left_entity}_{left_ent_id_edit})\n        LEFT JOIN rhs\n        USING({right_entity}_{right_ent_id_edit})\n        WHERE unit_1 IS NOT NULL\n        AND unit_2 IS NOT NULL\n        AND CAST(unit_1 AS VARCHAR) = CAST(unit_2 AS VARCHAR);\"\"\"\n\n    with duckdb.connect(database=db_path, read_only=False) as db_conn:\n        db_conn.execute(matching_query)\n        console.log(f\"[yellow] Created {match_name_col}\")\n        logger.debug(f\"Created {match_name_col}\")\n\n        execute_match_processing(\n            db_conn=db_conn,\n            link_table=link_table,\n            out_temp_table_name=temp_table,\n            id_col_1=f\"{left_entity}_{left_ent_id_edit}\",\n            match_name_col=match_name_col,\n            id_col_2=f\"{right_entity}_{right_ent_id_edit}\",\n        )\n\n    return None\n</code></pre>"},{"location":"modules/#chainlink.link.link_utils.generate_combos_within_across_tables","title":"<code>generate_combos_within_across_tables(name_idx, address_idx=None)</code>","text":"<p>create all possible combinations of across tables in the same entity, but do not include combos within the same table if address_idx is not empty, also create across combos between address tables</p> Source code in <code>src/chainlink/link/link_utils.py</code> <pre><code>def generate_combos_within_across_tables(name_idx: list, address_idx: Optional[list] = None) -&gt; tuple:\n    \"\"\"\n    create all possible combinations of across tables in the same entity,\n    but do not include combos within the same table\n    if address_idx is not empty, also create across combos between address tables\n    \"\"\"\n    if address_idx is None:\n        address_idx = []\n\n    across_combos_name_idx = list(itertools.combinations(range(len(name_idx)), 2))\n    across_name_combos: list = []\n    for i, j in across_combos_name_idx:\n        across_name_combos += itertools.product(name_idx[i], name_idx[j])\n        across_name_combos += itertools.product(name_idx[j], name_idx[i])\n\n    if len(address_idx) &gt; 0:\n        across_address_combos: list = []\n        across_combos_address_idx = list(itertools.combinations(range(len(address_idx)), 2))\n        for i, j in across_combos_address_idx:\n            across_address_combos += itertools.product(address_idx[i], address_idx[j])\n            across_address_combos += itertools.product(address_idx[j], address_idx[i])\n\n        return across_name_combos, across_address_combos\n\n    else:\n        return across_name_combos, []\n</code></pre>"},{"location":"modules/#chainlink.link.link_utils.generate_tfidf_links","title":"<code>generate_tfidf_links(db_path, table_location='entity.name_similarity', source_table_name=None)</code>","text":"<p>create a table of tfidf matches between two entities and adds to db</p> <p>Returns: None</p> Source code in <code>src/chainlink/link/link_utils.py</code> <pre><code>def generate_tfidf_links(\n    db_path: str | Path,\n    table_location: str = \"entity.name_similarity\",\n    source_table_name: str | None = None,\n) -&gt; None:\n    \"\"\"\n    create a table of tfidf matches between two entities and adds to db\n\n    Returns: None\n    \"\"\"\n\n    console.log(\"[yellow] Process started\")\n    logger.info(\"Process started\")\n\n    # retrieve entity list, print length of dataframe\n    entity_list = database_query(db_path, table_name=source_table_name)\n    console.log(f\"[yellow] Query retrieved {len(entity_list)} rows\")\n    logger.debug(f\"Query retrieved {len(entity_list)} rows\")\n\n    # returns a pandas df\n    entity_col = entity_list.columns[0]\n    id_col = entity_list.columns[1]\n    matches_df = superfast_tfidf(entity_list, id_col, entity_col)\n\n    console.log(\"[yellow] Fuzzy Matching done\")\n    logger.info(\"Fuzzy Matching done\")\n\n    # load back to db\n    with duckdb.connect(database=db_path, read_only=False) as db_conn:\n        query = f\"\"\"CREATE OR REPLACE TABLE {table_location} AS\n                    SELECT *\n                    FROM  matches_df\"\"\"\n\n        db_conn.execute(query)\n</code></pre>"},{"location":"modules/#chainlink.link.link_utils.query_append_to_links","title":"<code>query_append_to_links(link_table_exists, link_table, table_to_append, id_col1, id_col2)</code>","text":"<p>query to append links to link table runs in execute_match_processing()</p> Source code in <code>src/chainlink/link/link_utils.py</code> <pre><code>def query_append_to_links(\n    link_table_exists: bool,\n    link_table: str,\n    table_to_append: str,\n    id_col1: str,\n    id_col2: str,\n) -&gt; str:\n    \"\"\"\n    query to append links to link table\n    runs in execute_match_processing()\n    \"\"\"\n\n    # if link table does not exist then its just table to append\n    if link_table_exists:\n        query = f\"\"\"\n        CREATE OR REPLACE TABLE {link_table} AS\n        SELECT DISTINCT *\n        FROM {link_table}\n        FULL JOIN link.{table_to_append}\n        USING({id_col1}, {id_col2})\"\"\"\n\n    else:\n        query = f\"\"\"\n        CREATE OR REPLACE TABLE {link_table} AS\n        SELECT DISTINCT *\n        FROM link.{table_to_append}\"\"\"\n\n    return query\n</code></pre>"},{"location":"modules/#chainlink.link.tfidf_utils","title":"<code>tfidf_utils</code>","text":""},{"location":"modules/#chainlink.link.tfidf_utils.adjust_and_replace","title":"<code>adjust_and_replace(string)</code>","text":"<p>replace specified words with blanks and other words with their corresponding values for ngrams</p> Source code in <code>src/chainlink/link/tfidf_utils.py</code> <pre><code>def adjust_and_replace(string: str) -&gt; str:\n    \"\"\"\n    replace specified words with blanks and other words with their corresponding values for ngrams\n    \"\"\"\n\n    # remove punctuation\n    string = re.sub(r\"[,-./]\", r\"\", string)\n\n    # split the string into words\n    parts = string.split()\n\n    # replace words based on blank_words and flat_ngram_adj using list comprehension\n    adjusted_string = \"\".join([\"\" if part in blank_words else flat_ngram_adj.get(part, part) for part in parts])\n\n    return adjusted_string.strip()\n</code></pre>"},{"location":"modules/#chainlink.link.tfidf_utils.clean_matches","title":"<code>clean_matches(matches_df)</code>","text":"<p>remove self matches and duplicates in match dataframe</p> <p>Returns: pl.DataFrame</p> Source code in <code>src/chainlink/link/tfidf_utils.py</code> <pre><code>def clean_matches(matches_df: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"\n    remove self matches and duplicates in match dataframe\n\n    Returns: pl.DataFrame\n    \"\"\"\n\n    # create copy to make adjustments\n    # matches_df = matches_df.copy()\n    # remove self matches, duplicates and sort\n    matches_df = (\n        matches_df.filter(pl.col(\"id_a\") != pl.col(\"id_b\"))\n        .with_columns(pl.concat_list(pl.col(\"id_a\", \"id_b\")).list.sort().alias(\"sorted_id_pairs\"))\n        .unique(\"sorted_id_pairs\")\n        .drop(\"sorted_id_pairs\")\n        .sort(\"similarity\", descending=True)\n    )\n\n    return matches_df\n</code></pre>"},{"location":"modules/#chainlink.link.tfidf_utils.database_query","title":"<code>database_query(db_path, table_name=None, limit=None)</code>","text":"<p>queries entities for comparison</p> Source code in <code>src/chainlink/link/tfidf_utils.py</code> <pre><code>def database_query(db_path: str | Path, table_name: str | None = None, limit: int | None = None) -&gt; pl.DataFrame:\n    \"\"\"\n    queries entities for comparison\n    \"\"\"\n    if table_name is None:\n        table_name = \"entity.name\"\n        id_col = \"name_id\"\n    else:\n        id_col = table_name.split(\".\")[1] + \"_id\"\n\n    # start connection with woc db\n    with duckdb.connect(db_path) as conn:\n        entity_query = f\"\"\"\n        SELECT entity, {id_col}\n        FROM {table_name}\n        \"\"\"\n\n        # retreive entity list (all unique names in parcel, llc and corp data\n        entity_list = conn.execute(entity_query).pl()\n\n        # randomized sample for limit\n        if limit is not None:\n            entity_list = entity_list.sample(n=limit)\n\n    return entity_list\n</code></pre>"},{"location":"modules/#chainlink.link.tfidf_utils.get_matches_df","title":"<code>get_matches_df(sparse_matrix, name_vector, top=None)</code>","text":"<p>create a matches dataframe given matrix of ngrams references     sparse_matrix - matrix from vectorized comparison calculations     name_vector - list of names to compare     id_vector - id of distinct name from entities list</p> Source code in <code>src/chainlink/link/tfidf_utils.py</code> <pre><code>def get_matches_df(sparse_matrix: csr_matrix, name_vector: np.ndarray, top: None = None) -&gt; pl.DataFrame:\n    \"\"\"\n    create a matches dataframe given matrix of ngrams\n    references\n        sparse_matrix - matrix from vectorized comparison calculations\n        name_vector - list of names to compare\n        id_vector - id of distinct name from entities list\n    \"\"\"\n    non_zeros = sparse_matrix.nonzero()\n\n    sparserows = non_zeros[0]\n    sparsecols = non_zeros[1]\n\n    nr_matches = top if top else sparsecols.size\n\n    entity_a = np.empty([nr_matches], dtype=object)\n    entity_b = np.empty([nr_matches], dtype=object)\n    similarity = np.zeros(nr_matches)\n\n    for index in range(0, nr_matches):\n        entity_a[index] = name_vector[sparserows[index]]\n        entity_b[index] = name_vector[sparsecols[index]]\n        similarity[index] = sparse_matrix.data[index]\n\n    data = {\n        \"entity_a\": entity_a,\n        \"entity_b\": entity_b,\n        \"similarity\": similarity,\n    }\n    df = pl.DataFrame(data).with_columns(\n        pl.col(\"entity_a\").hash().alias(\"id_a\"), pl.col(\"entity_b\").hash().alias(\"id_b\")\n    )\n    return df\n</code></pre>"},{"location":"modules/#chainlink.link.tfidf_utils.ngrams","title":"<code>ngrams(string, n=3)</code>","text":"<p>split string into substrings of length n, return list of substrings</p> Source code in <code>src/chainlink/link/tfidf_utils.py</code> <pre><code>def ngrams(string: str, n: int = 3) -&gt; list:\n    \"\"\"\n    split string into substrings of length n, return list of substrings\n    \"\"\"\n    pre_processing = adjust_and_replace(string)\n    ngrams = zip(*[pre_processing[i:] for i in range(n)])\n    return [\"\".join(ngram) for ngram in ngrams]\n</code></pre>"},{"location":"modules/#chainlink.link.tfidf_utils.superfast_tfidf","title":"<code>superfast_tfidf(entity_list, id_col='name_id', entity_col='entity')</code>","text":"<p>returns sorted list of top matched names</p> Source code in <code>src/chainlink/link/tfidf_utils.py</code> <pre><code>def superfast_tfidf(entity_list: pl.DataFrame, id_col: str = \"name_id\", entity_col: str = \"entity\") -&gt; pl.DataFrame:\n    \"\"\"\n    returns sorted list of top matched names\n    \"\"\"\n\n    # matching\n\n    entity_list = entity_list.filter(~pl.col(entity_col).is_null())\n    company_names = entity_list.select(entity_col).to_series()\n    if len(company_names) &lt; 2:\n        matches_df = pl.DataFrame(data={\"entity_a\": [], \"entity_b\": [], \"similarity\": [], \"id_a\": [], \"id_b\": []})\n        return matches_df\n    vectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams)\n    tf_idf_matrix = vectorizer.fit_transform(company_names.to_numpy())\n    matches = ct.sp_matmul_topn(tf_idf_matrix, tf_idf_matrix.transpose(), 50, 0.8, sort=True, n_threads=-1)\n    matches_df = get_matches_df(sparse_matrix=matches, name_vector=company_names.to_numpy())\n    matches_df = clean_matches(matches_df)\n\n    return matches_df\n</code></pre>"},{"location":"modules/#chainlink.load","title":"<code>load</code>","text":""},{"location":"modules/#chainlink.load.load_generic","title":"<code>load_generic</code>","text":""},{"location":"modules/#chainlink.load.load_generic.load_generic","title":"<code>load_generic(db_path, schema_config, bad_addresses)</code>","text":"<p>Loads a generic file into the database.</p> <p>Reads config file, loops through each file listed, cleans the data, creates a unique id for name, street, and street_name, loads into cleaned files into a database using the schema name from the config file, and lastly updates the entity name files.</p> <p>Returns None.</p> Source code in <code>src/chainlink/load/load_generic.py</code> <pre><code>def load_generic(db_path: str | Path, schema_config: dict, bad_addresses: list) -&gt; None:\n    \"\"\"\n    Loads a generic file into the database.\n\n    Reads config file, loops through each file listed, cleans the data,\n    creates a unique id for name, street, and street_name,\n    loads into cleaned files into a database using the schema name from the config file,\n    and lastly updates the entity name files.\n\n    Returns None.\n    \"\"\"\n\n    schema_name = schema_config[\"schema_name\"]\n\n    with duckdb.connect(db_path, read_only=False) as conn:\n        for table_config in schema_config[\"tables\"]:\n            # Read the data\n            console.log(f\"[yellow] Data: {table_config['table_name']} -- Reading data\")\n            logger.info(f\"Data: {table_config['table_name']} -- Reading data\")\n            file_path = table_config.get(\"table_name_path\")\n            if not file_path:\n                raise ValueError(f\"No file path provided for table: {table_config['table_name']}\")\n\n            if not os.path.exists(file_path):\n                raise FileNotFoundError(f\"Data file not found: {file_path}\")\n\n            file_extension = file_path.split(\".\")[-1].lower()\n            if file_extension not in [\"csv\", \"parquet\"]:\n                raise ValueError(f\"Unsupported file format: {file_extension}. Supported formats: csv, parquet\")\n\n            try:\n                df = (\n                    pl.read_csv(file_path, infer_schema=False)\n                    if file_extension == \"csv\"\n                    else pl.read_parquet(file_path)\n                )\n                # convert all columns to string\n                df = df.cast(pl.String)\n\n            except Exception as e:\n                raise Exception(f\"Error reading file {file_path}: {e!s}\") from None\n\n            validate_input_data(df, table_config)\n\n            # Clean the data and create ids\n            console.log(f\"\"\"[yellow] Data: {table_config[\"table_name\"]} -- Starting cleaning\"\"\")\n            logger.info(f\"\"\"Data: {table_config[\"table_name\"]} -- Starting cleaning\"\"\")\n\n            all_columns = []\n            all_columns.append(table_config[\"id_col_og\"])\n            for col in table_config.get(\"name_cols_og\", \"\"):\n                all_columns.append(col)\n            for col in table_config.get(\"address_cols_og\", \"\"):\n                all_columns.append(col)\n\n            # Make headers snake case\n            df.columns = [x.lower().replace(\" \", \"_\") for x in df.columns]\n            # df.columns = df.columns.str, regex=True)\n\n            df = clean_generic(df, table_config)\n\n            # load the data to db\n            console.log(f\"\"\"[yellow] Data: {table_config[\"table_name\"]} -- Starting load\"\"\")\n\n            table_name = table_config[\"table_name\"]\n            load_to_db(\n                df=df,\n                table_name=table_name,\n                db_conn=conn,\n                schema=schema_name,\n            )\n\n            # add new names to entity_names table\n            console.log(f\"\"\"[yellow] Data: {table_config[\"table_name\"]} -- Updating entity name tables\"\"\")\n            logger.info(f\"\"\"Data: {table_config[\"table_name\"]} -- Updating entity name tables\"\"\")\n\n            all_id_cols = [\"name_id\", \"address_id\", \"street_id\", \"street_name_id\"]\n\n            id_cols = []\n            for col in df.columns:\n                if any(c in col for c in all_id_cols) and \"subaddress_identifier\" not in col:\n                    id_cols.append(col)\n\n            for col in id_cols:\n                update_entity_ids(df=df, entity_id_col=col, db_conn=conn)\n\n            # create bad address flag\n            if table_config.get(\"address_cols\"):\n                for col in table_config[\"address_cols\"]:\n                    execute_flag_bad_addresses(\n                        db_conn=conn,\n                        table=f\"{schema_name}.{table_name}\",\n                        address_col=col,\n                        bad_addresses=bad_addresses,\n                    )\n</code></pre>"},{"location":"modules/#chainlink.load.load_utils","title":"<code>load_utils</code>","text":""},{"location":"modules/#chainlink.load.load_utils.clean_generic","title":"<code>clean_generic(df, config)</code>","text":"<p>Cleans the name and address for a generic file. Appends a new column with the cleaned name and address.</p> <p>Returns a pl.DataFrame</p> Source code in <code>src/chainlink/load/load_utils.py</code> <pre><code>def clean_generic(df: pl.DataFrame, config: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Cleans the name and address for a generic file. Appends a new\n    column with the cleaned name and address.\n\n    Returns a pl.DataFrame\n    \"\"\"\n\n    # Clean the name\n    for col in config[\"name_cols\"]:\n        # lower snake case\n        col = col.lower().replace(\" \", \"_\")\n\n        raw_name = col + \"_raw\"\n        id_col_name = col + \"_name\"\n\n        # weird case TODO\n        if raw_name in df.columns:\n            df = df.drop(columns=[raw_name])\n        df = (\n            df.rename({col: raw_name})\n            .with_columns(\n                pl.col(raw_name)\n                .fill_null(\"\")\n                .str.to_uppercase()\n                .map_elements(clean_names, return_dtype=pl.String)\n                .alias(col)\n            )\n            .with_columns(pl.col(col).alias(id_col_name))\n        )\n\n        df = create_id_col(df, id_col_name)\n        df = df.drop(id_col_name)\n\n    # Clean the address\n    if config.get(\"address_cols\"):\n        for col in config[\"address_cols\"]:\n            # lower snake case\n            col = col.lower().replace(\" \", \"_\")\n\n            raw_address = col + \"_raw\"\n            temp_address = \"temp_\" + col\n            console.log(f\"[yellow] Cleaning address column {col}\")\n\n            df = df.with_columns(\n                pl.col(col).fill_null(\"\").str.to_uppercase().alias(raw_address),\n                pl.col(col).fill_null(\"\").str.to_uppercase().alias(temp_address),\n            )\n            df = df.with_columns(\n                pl.col(temp_address).map_elements(\n                    clean_address,\n                    return_dtype=pl.Struct([\n                        pl.Field(\"raw\", pl.Utf8),\n                        pl.Field(\"address_number\", pl.Utf8),\n                        pl.Field(\"street_pre_directional\", pl.Utf8),\n                        pl.Field(\"street_name\", pl.Utf8),\n                        pl.Field(\"street_post_type\", pl.Utf8),\n                        pl.Field(\"unit_type\", pl.Utf8),\n                        pl.Field(\"unit_number\", pl.Utf8),\n                        pl.Field(\"subaddress_type\", pl.Utf8),\n                        pl.Field(\"subaddress_identifier\", pl.Utf8),\n                        pl.Field(\"city\", pl.Utf8),\n                        pl.Field(\"state\", pl.Utf8),\n                        pl.Field(\"postal_code\", pl.Utf8),\n                        pl.Field(\"street\", pl.Utf8),\n                        pl.Field(\"address_norm\", pl.Utf8),\n                    ]),\n                )\n            )\n            ta_fields = df[temp_address].struct.fields\n            new_fields = [f\"{col}_{f}\" for f in ta_fields]\n            df = (\n                df.drop(raw_address)\n                .with_columns(pl.col(temp_address).struct.rename_fields(new_fields))\n                .unnest(temp_address)\n                .with_columns(\n                    pl.col(f\"{col}_postal_code\").cast(pl.String).map_elements(clean_zipcode, return_dtype=pl.String),\n                    pl.col(f\"{col}_address_norm\").cast(pl.String).alias(col + \"_address\"),\n                )\n                .with_columns(pl.col(col + \"_address\").replace(\"\", None))\n            )\n\n            id_cols = [\"address\", \"street\", \"street_name\"]\n\n            # create id col\n            for id_col in id_cols:\n                name = col + \"_\" + id_col\n                df = create_id_col(df, name)\n\n        # drop temp cols\n        df = df.drop(col + \"_address\")\n    return df\n</code></pre>"},{"location":"modules/#chainlink.load.load_utils.create_id_col","title":"<code>create_id_col(df, col)</code>","text":"<p>Adds an id column to the DataFrame using pl.Series.hash function</p> <p>Returns a pl.DataFrame</p> Source code in <code>src/chainlink/load/load_utils.py</code> <pre><code>def create_id_col(df: pl.DataFrame, col: str) -&gt; pl.DataFrame:\n    \"\"\"\n    Adds an id column to the DataFrame using pl.Series.hash function\n\n    Returns a pl.DataFrame\n    \"\"\"\n\n    col_id = col + \"_id\"\n\n    df = df.with_columns(pl.col(col).hash().alias(col_id)).with_columns(\n        pl.when(pl.col(col).is_null()).then(None).otherwise(pl.col(col_id)).alias(col_id).cast(pl.UInt64),\n    )\n\n    return df\n</code></pre>"},{"location":"modules/#chainlink.load.load_utils.execute_flag_bad_addresses","title":"<code>execute_flag_bad_addresses(db_conn, table, address_col, bad_addresses)</code>","text":"<p>Flags rows with bad addresses as provided by user</p> Source code in <code>src/chainlink/load/load_utils.py</code> <pre><code>def execute_flag_bad_addresses(db_conn: DuckDBPyConnection, table: str, address_col: str, bad_addresses: list) -&gt; None:\n    \"\"\"\n    Flags rows with bad addresses as provided by user\n    \"\"\"\n    console.log(f\"[yellow] Flagging bad addresses in {table} table for {address_col} column\")\n    if bad_addresses:\n        bad_addresses_tuple = tuple(bad_addresses)\n\n        query = f\"\"\"\n                CREATE OR REPLACE TABLE {table} AS\n                SELECT *,\n                        CASE WHEN\n                            ({address_col} in {bad_addresses_tuple}\n                            OR {address_col}_street in {bad_addresses_tuple}) THEN 1\n                        ELSE 0 END as {address_col}_skip\n                from {table}\n                \"\"\"\n\n    else:\n        query = f\"\"\"\n            CREATE OR REPLACE TABLE {table} AS\n            SELECT *, 0 as {address_col}_skip\n            from {table}\n            \"\"\"\n        console.log(f\"[yellow] No bad addresses to flag in {table} table for {address_col} column\")\n\n    db_conn.execute(query)\n    return None\n</code></pre>"},{"location":"modules/#chainlink.load.load_utils.load_to_db","title":"<code>load_to_db(df, table_name, db_conn, schema)</code>","text":"<p>Loads parquet file into table in database.</p>"},{"location":"modules/#chainlink.load.load_utils.load_to_db--parameters","title":"Parameters","text":"<p>filepath : str     Directory of parquet file to load onto database. table_name : str     Name of resulting table in database. db_conn : object     Connection object to desired duckdb database. schema : str     Name of schema for resulting table in database.</p>"},{"location":"modules/#chainlink.load.load_utils.load_to_db--returns","title":"Returns","text":"<p>None</p> Source code in <code>src/chainlink/load/load_utils.py</code> <pre><code>def load_to_db(df: pl.DataFrame, table_name: str, db_conn: DuckDBPyConnection, schema: str) -&gt; None:\n    \"\"\"Loads parquet file into table in database.\n\n    Parameters\n    ----------\n    filepath : str\n        Directory of parquet file to load onto database.\n    table_name : str\n        Name of resulting table in database.\n    db_conn : object\n        Connection object to desired duckdb database.\n    schema : str\n        Name of schema for resulting table in database.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    df = df\n    query = f\"\"\"\n            CREATE SCHEMA IF NOT EXISTS {schema};\n            DROP TABLE IF EXISTS {schema}.{table_name};\n            CREATE TABLE {schema}.{table_name} AS\n               SELECT *\n               FROM df;\n            \"\"\"\n\n    db_conn.execute(query)\n</code></pre>"},{"location":"modules/#chainlink.load.load_utils.update_entity_ids","title":"<code>update_entity_ids(df, entity_id_col, db_conn)</code>","text":"<p>Adds new ids to the entity schema table. If the value is already in the table, it is not added.</p> <p>Returns None</p> Source code in <code>src/chainlink/load/load_utils.py</code> <pre><code>def update_entity_ids(df: pl.DataFrame, entity_id_col: str, db_conn: DuckDBPyConnection) -&gt; None:\n    \"\"\"\n    Adds new ids to the entity schema table. If the value is already in the table, it is not added.\n\n    Returns None\n    \"\"\"\n\n    split_col = entity_id_col.split(\"_\")\n    if \"_\".join(split_col[-3:]) == \"street_name_id\":\n        entity_table_name = \"street_name\"\n        entity_col = entity_id_col.split(\"_id\")[0]\n    elif \"_\".join(split_col[-2:]) == \"street_id\":\n        entity_table_name = \"street\"\n        entity_col = entity_id_col.split(\"_id\")[0]\n    elif \"_\".join(split_col[-2:]) == \"address_id\":\n        entity_col = entity_id_col.replace(\"_address_id\", \"\")\n        entity_table_name = \"address\"\n    elif \"_\".join(split_col[-2:]) == \"name_id\":\n        entity_col = entity_id_col.replace(\"_name_id\", \"\")\n        entity_table_name = \"name\"\n\n    if not check_table_exists(db_conn, \"entity\", entity_table_name):\n        # a check if entity tables doesnt exist, just creates it\n        query = f\"\"\"\n                CREATE SCHEMA IF NOT EXISTS entity;\n\n                CREATE TABLE entity.{entity_table_name} AS\n                    SELECT {entity_col} as entity,\n                           {entity_id_col} as {entity_table_name + \"_id\"}\n                    from  df\n                ;\n                \"\"\"\n\n    else:\n        # otherwise, add any new entities to the existing table\n\n        query = f\"\"\"\n                CREATE OR REPLACE TABLE entity.{entity_table_name} AS (\n\n\n                SELECT {entity_col} as entity,\n                       {entity_id_col} as {entity_table_name + \"_id\"}\n                from  df\n\n                UNION DISTINCT\n\n                select entity,\n                       {entity_table_name + \"_id\"}\n                from   entity.{entity_table_name}\n                )\n                \"\"\"\n    db_conn.execute(query)\n\n    return None\n</code></pre>"},{"location":"modules/#chainlink.load.load_utils.validate_input_data","title":"<code>validate_input_data(df, table_config)</code>","text":"<p>Validates input data against configuration requirements</p> Source code in <code>src/chainlink/load/load_utils.py</code> <pre><code>def validate_input_data(df: pl.DataFrame, table_config: dict) -&gt; None:\n    \"\"\"\n    Validates input data against configuration requirements\n    \"\"\"\n    required_columns = set()\n    required_columns.add(table_config[\"id_col_og\"])\n    if table_config.get(\"name_cols_og\"):\n        required_columns.update(table_config[\"name_cols_og\"])\n    if table_config.get(\"address_cols_og\"):\n        required_columns.update(table_config[\"address_cols_og\"])\n\n    missing_columns = required_columns - set(df.columns)\n    if missing_columns:\n        raise ValueError(f\"Missing required columns: {', '.join(missing_columns)}\")\n\n    # Check for empty dataframe\n    if df.is_empty():\n        raise ValueError(\"Input data is empty\")\n\n    # Check for minimum required non-null values\n    for col in required_columns:\n        null_count = df.select(pl.col(col).is_null().sum()).item()\n        if null_count == len(df):\n            raise ValueError(f\"Column {col} contains all null values\")\n</code></pre>"},{"location":"modules/#chainlink.main","title":"<code>main</code>","text":""},{"location":"modules/#chainlink.main.chainlink","title":"<code>chainlink(config, config_path=DIR / 'configs/config.yaml')</code>","text":"<p>Given a correctly formatted config file,     * load in any schemas in the config that are not already in the database     * create within links for each new schema     * create across links for each new schema with all existing schemas</p> <p>Returns true if the database was created successfully.</p> Source code in <code>src/chainlink/main.py</code> <pre><code>def chainlink(\n    config: dict,\n    config_path: str | Path = DIR / \"configs/config.yaml\",\n) -&gt; bool:\n    \"\"\"\n    Given a correctly formatted config file,\n        * load in any schemas in the config that are not already in the database\n        * create within links for each new schema\n        * create across links for each new schema with all existing schemas\n\n\n    Returns true if the database was created successfully.\n    \"\"\"\n    probabilistic = config[\"options\"].get(\"probabilistic\", False)\n    load_only = config[\"options\"].get(\"load_only\", False)\n    db_path = config[\"options\"].get(\"db_path\", DIR / \"db/linked.db\")\n\n    no_names = True\n    no_addresses = True\n\n    # create snake case columns\n    for schema in config[\"schemas\"]:\n        for table in schema[\"tables\"]:\n            if len(table[\"name_cols\"]) &gt; 0:\n                no_names = False\n                table[\"name_cols_og\"] = table[\"name_cols\"]\n                table[\"name_cols\"] = [x.lower().replace(\" \", \"_\") for x in table[\"name_cols\"]]\n            else:\n                table[\"name_cols\"] = []\n\n            if len(table[\"address_cols\"]) &gt; 0:\n                no_addresses = False\n                table[\"address_cols_og\"] = table[\"address_cols\"]\n                table[\"address_cols\"] = [x.lower().replace(\" \", \"_\") for x in table[\"address_cols\"]]\n            else:\n                table[\"address_cols\"] = []\n\n            table[\"id_col_og\"] = table[\"id_col\"]\n            table[\"id_col\"] = table[\"id_col\"].lower().replace(\" \", \"_\")\n\n    # handle options\n    overwrite_db = config[\"options\"].get(\"overwrite_db\", False)\n    if overwrite_db and os.path.exists(db_path):\n        os.remove(db_path)\n        console.print(f\"[red] Removed existing database at {db_path}\")\n        logger.info(f\"Removed existing database at {db_path}\")\n\n    update_config_only = config[\"options\"].get(\"update_config_only\", False)\n    if update_config_only:\n        update_config(db_path, config, config_path)\n        return True\n\n    bad_address_path = config[\"options\"].get(\"bad_address_path\", None)\n    if bad_address_path is not None:\n        try:\n            bad_addresses_df = pl.read_csv(bad_address_path)\n            bad_addresses = bad_addresses_df[:, 0].to_list()\n        except Exception:\n            bad_addresses = []\n    else:\n        bad_addresses = []\n\n    # list of link exclusions\n\n    link_exclusions = config[\"options\"].get(\"link_exclusions\", None)\n    if not link_exclusions:\n        link_exclusions = []\n\n    # all columns in db to compare against\n    with duckdb.connect(database=db_path, read_only=False) as con:\n        df_db_columns = con.sql(\"show all tables\").pl()\n\n    schemas = config[\"schemas\"]\n    new_schemas = []\n\n    # load each schema. if schema is a new entity, create links\n    for schema_config in schemas:\n        schema_name = schema_config[\"schema_name\"]\n\n        # if not force create, check if each col exists, and skip if so\n        if not overwrite_db:\n            if df_db_columns.filter(pl.col(\"schema\") == schema_name).shape[0] == 0:\n                new_schemas.append(schema_name)\n        else:\n            new_schemas.append(schema_name)\n\n    # load in all new schemas\n    for new_schema in new_schemas:\n        schema_config = [schema for schema in schemas if schema[\"schema_name\"] == new_schema][0]\n\n        with console.status(f\"[bold yellow] Working on loading {new_schema}\") as status:\n            # load schema\n            load_generic(\n                db_path=db_path,\n                schema_config=schema_config,\n                bad_addresses=bad_addresses,\n            )\n\n        if not load_only:\n            # create exact links\n            with console.status(f\"[bold yellow] Working on linking {new_schema}\") as status:\n                create_within_links(\n                    db_path=db_path,\n                    schema_config=schema_config,\n                    link_exclusions=link_exclusions,\n                )\n\n    if not load_only and probabilistic:\n        #  generate all the fuzzy links and store in entity.name_similarity\n        # only if there are new schemas added\n        if len(new_schemas) &gt; 0:\n            with console.status(\"[bold yellow] Working on fuzzy matching scores\") as status:\n                if not no_names:\n                    generate_tfidf_links(db_path, table_location=\"entity.name_similarity\")\n                if not no_addresses:\n                    generate_tfidf_links(\n                        db_path,\n                        table_location=\"entity.street_name_similarity\",\n                        source_table_name=\"entity.street_name\",\n                    )\n\n        # for across link\n        links = []\n        created_schemas = []\n\n        # create tfidf links within each new schema\n        for new_schema in new_schemas:\n            schema_config = [schema for schema in schemas if schema[\"schema_name\"] == new_schema][0]\n\n            if probabilistic:\n                with console.status(f\"[bold yellow] Working on fuzzy matching links in {new_schema}\") as status:\n                    create_tfidf_within_links(\n                        db_path=db_path,\n                        schema_config=schema_config,\n                        link_exclusions=link_exclusions,\n                    )\n\n            # also create across links for each new schema\n            existing_schemas = [schema for schema in schemas if schema[\"schema_name\"] != new_schema]\n\n            new_schema_config = [schema for schema in schemas if schema[\"schema_name\"] == new_schema][0]\n\n            # make sure we havent already created this link combo\n            for schema in existing_schemas:\n                if sorted(new_schema + schema[\"schema_name\"]) not in created_schemas:\n                    links.append((new_schema_config, schema))\n                    created_schemas.append(sorted(new_schema + schema[\"schema_name\"]))\n\n        # across links for each new_schema, link across to all existing entities\n        for new_schema_config, existing_schema in links:\n            with console.status(\n                f\"[bold yellow] Working on links between {new_schema_config['schema_name']} and {existing_schema['schema_name']}\"\n            ) as status:\n                create_across_links(\n                    db_path=db_path,\n                    new_schema=new_schema_config,\n                    existing_schema=existing_schema,\n                    link_exclusions=link_exclusions,\n                )\n\n            if probabilistic:\n                with console.status(\n                    f\"[bold yellow] Working on fuzzy links between {new_schema_config['schema_name']} and {existing_schema['schema_name']}\"\n                ) as status:\n                    create_tfidf_across_links(\n                        db_path=db_path,\n                        new_schema=new_schema_config,\n                        existing_schema=existing_schema,\n                        link_exclusions=link_exclusions,\n                    )\n\n    update_config(db_path, config, config_path)\n\n    export_tables_flag = config[\"options\"].get(\"export_tables\", False)\n    if export_tables_flag:\n        path = DIR / \"data\" / \"export\"\n        export_tables(db_path, path)\n\n    return True  ## TODO: check if this is true or false\n</code></pre>"},{"location":"modules/#chainlink.main.main","title":"<code>main(config=typer.Argument(DIR / 'config' / 'chainlink_config.yaml', exists=True, readable=True))</code>","text":"<p>Given a correctly formatted config file,     * load in any schemas in the config that are not already in the database     * create within links for each new schema     * create across links for each new schema with all existing schemas</p> <p>Returns 'True' if the database was created successfully.</p> Source code in <code>src/chainlink/main.py</code> <pre><code>@app.command()\ndef main(config: str = typer.Argument(DIR / \"config\" / \"chainlink_config.yaml\", exists=True, readable=True)) -&gt; None:\n    \"\"\"\n    Given a correctly formatted config file,\n        * load in any schemas in the config that are not already in the database\n        * create within links for each new schema\n        * create across links for each new schema with all existing schemas\n\n    Returns 'True' if the database was created successfully.\n    \"\"\"\n    config_dict = load_config(config) if config is not None and os.path.exists(config) else create_config()\n    chainlink(config_dict, config_path=config)\n\n    console.print(\"[green bold] chainlink complete, database created\")\n    logger.info(\"chainlink complete, database created\")\n</code></pre>"},{"location":"modules/#chainlink.utils","title":"<code>utils</code>","text":""},{"location":"modules/#chainlink.utils.add_schema_config","title":"<code>add_schema_config(config)</code>","text":"<p>Helper to add a schema to an existing config</p> Source code in <code>src/chainlink/utils.py</code> <pre><code>def add_schema_config(config: dict) -&gt; dict:\n    \"\"\"\n    Helper to add a schema to an existing config\n    \"\"\"\n\n    schema_name = Prompt.ask(\"[green]&gt; Enter the name of the schema\", default=\"main\")\n    config[\"schemas\"].append({\"schema_name\": schema_name, \"tables\": []})\n    config = add_table_config(config, schema_name)\n    add_table = Confirm.ask(\"[green]&gt; Add a table to this schema?\", default=True, show_default=True)\n    while add_table:\n        config = add_table_config(config, schema_name)\n        add_table = Confirm.ask(\n            \"[green]&gt; Add another table to this schema?\",\n            default=False,\n            show_default=True,\n        )\n    console.print(\"[green italic]&gt; Schema added successfully!\")\n    return config\n</code></pre>"},{"location":"modules/#chainlink.utils.add_table_config","title":"<code>add_table_config(config, schema_name)</code>","text":"<p>Helper to add a table to an existing schema</p> Source code in <code>src/chainlink/utils.py</code> <pre><code>def add_table_config(config: dict, schema_name: str) -&gt; dict:\n    \"\"\"\n    Helper to add a table to an existing schema\n    \"\"\"\n\n    table_name = Prompt.ask(\"[green]&gt; Enter the name of dataset:\", default=\"dataset\", show_default=True)\n    table_name = table_name.lower().replace(\" \", \"_\")\n    table_name_path = Prompt.ask(\"[green]&gt; Enter the path to the dataset\")\n    while not os.path.exists(table_name_path):\n        table_name_path = Prompt.ask(\"[red]&gt; Path does not exist. Please enter a valid path\")\n    id_col = Prompt.ask(\"[green]&gt; Enter the id column of the dataset. Must be unique\")\n    name_col_str = Prompt.ask(\"[green]&gt; Enter the name column(s) (comma separated)\")\n    name_cols = [_.strip() for _ in name_col_str.split(\",\")]\n    address_col_str = Prompt.ask(\"[green]&gt; Enter the address column(s) (comma separated)\")\n    address_cols = [_.strip() for _ in address_col_str.split(\",\")]\n\n    for idx, schema in enumerate(config[\"schemas\"]):\n        if schema[\"schema_name\"] == schema_name:\n            config[\"schemas\"][idx][\"tables\"].append({\n                \"table_name\": table_name,\n                \"table_name_path\": table_name_path,\n                \"id_col\": id_col,\n                \"name_cols\": name_cols,\n                \"address_cols\": address_cols,\n            })\n\n    return config\n</code></pre>"},{"location":"modules/#chainlink.utils.check_table_exists","title":"<code>check_table_exists(db_conn, schema, table_name)</code>","text":"<p>check if a table exists</p> <p>Returns: bool</p> Source code in <code>src/chainlink/utils.py</code> <pre><code>def check_table_exists(db_conn: DuckDBPyConnection, schema: str, table_name: str) -&gt; bool:\n    \"\"\"\n    check if a table exists\n\n    Returns: bool\n    \"\"\"\n\n    db_conn.execute(\n        f\"\"\"    SELECT COUNT(*)\n                FROM   information_schema.tables\n                WHERE  table_name = '{table_name}'\n                AND    table_schema = '{schema}'\"\"\"\n    )\n\n    return db_conn.fetchone()[0] == 1\n</code></pre>"},{"location":"modules/#chainlink.utils.create_config","title":"<code>create_config()</code>","text":"<p>Helper to create config file from user input if not pre created</p> Source code in <code>src/chainlink/utils.py</code> <pre><code>def create_config() -&gt; dict:\n    \"\"\"\n    Helper to create config file from user input if not pre created\n    \"\"\"\n    create_config_path = Prompt.ask(\n        \"[green]&gt; Enter config path. [Leave blank if you would you like to create a new one]\",\n        default=\"\",\n        show_default=False,\n    )\n    create_config_path = create_config_path.strip()\n    if create_config_path.lower() != \"\":\n        while not os.path.exists(create_config_path):\n            create_config_path = Prompt.ask(\"[red]&gt; Yaml path does not exist. Please enter a valid path\")\n            create_config_path = create_config_path.strip()\n\n        config = load_config(create_config_path)\n\n        while True:\n            if validate_config(config):\n                break\n            else:  # invalid config\n                # print(validate_config(config))\n                create_config_path = Prompt.ask(\"[red]&gt; Invalid config. Please enter a valid yaml config\")\n                create_config_path = input().strip()\n                config = load_config(create_config_path)\n\n        return config\n    else:\n        config = {\n            \"options\": {\n                \"overwrite_db\": False,\n                \"export_tables\": False,\n                \"update_config_only\": False,\n                \"link_exclusions\": [],\n                \"bad_address_path\": None,\n                \"probabilistic\": False,\n                \"load_only\": False,\n            },\n            \"schemas\": [],\n        }\n        # build config with user input\n        config[\"options\"][\"db_path\"] = Prompt.ask(\n            \"[green]&gt; Enter the path to the resulting database\",\n            default=\"db/linked.db\",\n            show_default=True,\n        )\n\n        config[\"options\"][\"load_only\"] = Confirm.ask(\n            \"[green]&gt; Only clean and load data to the database (without matching)?\",\n            show_default=True,\n            default=False,\n        )\n\n        if not config[\"options\"][\"load_only\"]:\n            config[\"options\"][\"probablistic\"] = Confirm.ask(\n                \"[green]&gt; Run probabilisitic name and address matching?\",\n                show_default=True,\n                default=False,\n            )\n\n        config[\"options\"][\"export_tables\"] = Confirm.ask(\n            \"[green]&gt; Export tables to parquet after load?\",\n            show_default=True,\n            default=False,\n        )\n\n        bad_address_path = Prompt.ask(\n            \"[dim green]&gt; [Optional] Provide path to bad address csv file\",\n            default=\"\",\n            show_default=False,\n        )\n        bad_address_path = bad_address_path.strip()\n        if bad_address_path:\n            while not os.path.exists(bad_address_path):\n                console.print(\"&gt; Bad address path does not exist. Please enter a valid path or leave blank:\")\n                bad_address_path = input().strip()\n            config[\"options\"][\"bad_address_path\"] = bad_address_path\n\n        add_schema = Confirm.ask(\"[green]&gt; Add a new schema?\", default=True, show_default=True)\n        while add_schema:\n            config = add_schema_config(config)\n            add_schema = Confirm.ask(\"&gt; Add another schema?\", default=False, show_default=True)\n\n        return config\n</code></pre>"},{"location":"modules/#chainlink.utils.export_tables","title":"<code>export_tables(db_path, data_path)</code>","text":"<p>export all tables from database to parquet files in {data_path}/export directory</p> <p>Returns: None</p> Source code in <code>src/chainlink/utils.py</code> <pre><code>def export_tables(db_path: str | Path, data_path: str | Path) -&gt; None:\n    \"\"\"\n    export all tables from database to parquet files in {data_path}/export directory\n\n    Returns: None\n    \"\"\"\n\n    # create export directory if doesn't exist\n    if not os.path.exists(data_path):\n        os.makedirs(data_path)\n\n    def find_id_cols(row: dict) -&gt; list:  # TODO: check if this is correct\n        if row[\"schema\"] == \"link\" or row[\"name\"] == \"name_similarity\":\n            return row[\"column_names\"][:2]\n        elif row[\"schema\"] == \"entity\":\n            return [row[\"column_names\"][1]]\n        else:\n            return [row[\"column_names\"][0]]\n\n    with duckdb.connect(db_path) as conn:\n        df_db_columns = conn.sql(\"show all tables\").pl()\n\n        df_db_columns = df_db_columns.with_columns(\n            schema_table=pl.col(\"schema\") + \".\" + pl.col(\"name\"),\n            id_col=pl.struct(pl.all()).map_elements(lambda x: find_id_cols(x), return_dtype=pl.List(pl.String)),\n        )\n        link_filter = (pl.col(\"schema\") == \"link\") | (pl.col(\"name\") == \"name_similarity\")\n\n        links_to_export = zip(\n            df_db_columns.filter(link_filter)[\"schema_table\"].to_list(),\n            df_db_columns.filter(link_filter)[\"id_col\"].to_list(),\n        )\n\n        for link in links_to_export:\n            links_query = f\"\"\"\n                (select * from {link[0]}\n                order by {link[1][0]} ASC, {link[1][1]} ASC);\n            \"\"\"\n            d = conn.execute(links_query).pl().cast({link[1][0]: pl.String, link[1][1]: pl.String})\n            d.write_parquet(f\"{data_path}/{link[0].replace('.', '_')}.parquet\")\n\n        main_filter = (pl.col(\"schema\") != \"link\") &amp; (pl.col(\"name\") != \"name_similarity\")\n        print(main_filter)\n        main_to_export = zip(\n            df_db_columns.filter(main_filter)[\"schema_table\"].to_list(),\n            df_db_columns.filter(main_filter)[\"id_col\"].to_list(),\n        )\n\n        for table, id_cols in main_to_export:\n            sql_to_exec = f\"\"\"\n                (select * from {table}\n                order by {id_cols[0]} ASC);\n            \"\"\"\n            d = conn.execute(sql_to_exec).pl().cast({id_cols[0]: pl.String})\n            d.write_parquet(f\"{data_path}/{table.replace('.', '_')}.parquet\")\n\n    print(\"Exported all tables!\")\n    logger.info(\"Exported all tables!\")\n</code></pre>"},{"location":"modules/#chainlink.utils.load_config","title":"<code>load_config(file_path)</code>","text":"<p>load yaml config file, clean up column names</p> <p>Returns: dict</p> Source code in <code>src/chainlink/utils.py</code> <pre><code>def load_config(file_path: str) -&gt; dict:\n    \"\"\"\n    load yaml config file, clean up column names\n\n    Returns: dict\n    \"\"\"\n\n    with open(file_path) as file:\n        config = yaml.safe_load(file)\n\n    return config\n</code></pre>"},{"location":"modules/#chainlink.utils.setup_logger","title":"<code>setup_logger(name, log_file, level=logging.DEBUG)</code>","text":"<p>To setup as many loggers as you want</p>"},{"location":"modules/#chainlink.utils.setup_logger--from-httpsstackoverflowcomquestions11232230logging-to-two-files-with-different-settings","title":"from https://stackoverflow.com/questions/11232230/logging-to-two-files-with-different-settings","text":"Source code in <code>src/chainlink/utils.py</code> <pre><code>def setup_logger(name: str, log_file: str, level: int | str = logging.DEBUG) -&gt; logging.Logger:\n    \"\"\"\n    To setup as many loggers as you want\n    # from https://stackoverflow.com/questions/11232230/logging-to-two-files-with-different-settings\n    \"\"\"\n\n    formatter = logging.Formatter(\"%(asctime)s %(levelname)s %(message)s\")\n\n    handler = logging.FileHandler(log_file)\n    handler.setFormatter(formatter)\n\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n    logger.addHandler(handler)\n\n    return logger\n</code></pre>"},{"location":"modules/#chainlink.utils.update_config","title":"<code>update_config(db_path, config, config_path)</code>","text":"<p>update config by adding in all existing link columns and last updated time. writes config back out to config.yaml</p> <p>Returns: None</p> Source code in <code>src/chainlink/utils.py</code> <pre><code>def update_config(db_path: str | Path, config: dict, config_path: str | Path) -&gt; None:\n    \"\"\"\n    update config by adding in all existing link columns and last updated time.\n    writes config back out to config.yaml\n\n    Returns: None\n    \"\"\"\n\n    with duckdb.connect(db_path) as conn:\n        df_db_columns = conn.sql(\"show all tables\").pl()\n\n    all_links = []\n    for cols in df_db_columns[\"column_names\"].to_list():\n        all_links += [col for col in cols if \"match\" in col]\n\n    if \"metadata\" not in config:\n        config[\"metadata\"] = {}\n\n    config[\"metadata\"][\"existing_links\"] = all_links\n    config[\"metadata\"][\"last_updated\"] = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    with open(config_path, \"w+\") as f:\n        yaml.dump(config, f)\n</code></pre>"},{"location":"modules/#chainlink.utils.validate_config","title":"<code>validate_config(config)</code>","text":"<p>Validates the configuration against a schema</p> Source code in <code>src/chainlink/utils.py</code> <pre><code>def validate_config(config: dict) -&gt; bool:\n    \"\"\"\n    Validates the configuration against a schema\n    \"\"\"\n    schema = {\n        \"type\": \"object\",\n        \"required\": [\"options\", \"schemas\"],\n        \"properties\": {\n            \"options\": {\n                \"type\": \"object\",\n                \"required\": [\"db_path\"],\n                \"properties\": {\n                    \"overwrite_db\": {\"type\": \"boolean\"},\n                    \"export_tables\": {\"type\": \"boolean\"},\n                    \"update_config_only\": {\"type\": \"boolean\"},\n                    \"link_exclusions\": {\"type\": [\"array\", \"null\"]},  # or none\n                    \"bad_address_path\": {\"type\": \"string\"},  # or none\n                },\n            },\n            \"schemas\": {\n                \"type\": \"array\",\n                \"items\": {\n                    \"type\": \"object\",\n                    \"required\": [\"schema_name\", \"tables\"],\n                    \"properties\": {\n                        \"schema_name\": {\"type\": \"string\"},\n                        \"tables\": {\n                            \"type\": \"array\",\n                            \"items\": {\n                                \"type\": \"object\",\n                                \"required\": [\"table_name\", \"table_name_path\", \"id_col\"],\n                                \"properties\": {\n                                    \"table_name\": {\"type\": \"string\"},\n                                    \"table_name_path\": {\"type\": \"string\"},\n                                    \"id_col\": {\"type\": \"string\"},\n                                    \"name_cols\": {\n                                        \"type\": [\"array\", \"null\"],\n                                        \"items\": {\"type\": \"string\"},\n                                    },\n                                    \"address_cols\": {\n                                        \"type\": [\"array\", \"null\"],\n                                        \"items\": {\"type\": \"string\"},\n                                    },\n                                },\n                            },\n                        },\n                    },\n                },\n            },\n        },\n    }\n\n    try:\n        jsonschema.validate(instance=config, schema=schema)\n    except jsonschema.exceptions.ValidationError as e:\n        console.print(f\"[bold red]&gt; Invalid configuration: {e!s}\")\n        return False\n\n    # ids across tables but within schema should be the same\n    for schema in config[\"schemas\"]:\n        ids = set()\n        for table in schema[\"tables\"]:\n            ids.add(table[\"id_col\"])\n\n        if len(ids) != 1:\n            console.print(f\"[bold red]&gt; All tables in schema {schema['schema_name']} must have the same id column\")\n            return False\n\n    # no exception\n    return True\n</code></pre>"},{"location":"output-db-schema/","title":"Output Database Schema","text":""},{"location":"output-db-schema/#database-schema","title":"Database Schema","text":"<p>The framework creates a DuckDB database with the following schema structure:</p>"},{"location":"output-db-schema/#schemas","title":"Schemas","text":"<ol> <li>entity: Contains standardized entity information<ul> <li><code>name</code>: Unique entity names with IDs</li> <li><code>address</code>: Unique addresses with IDs</li> <li><code>street</code>: Unique street information with IDs</li> <li><code>street_name</code>: Unique street names with IDs</li> <li><code>name_similarity</code>: TF-IDF similarity scores between entity names</li> <li><code>street_name_similarity</code>: TF-IDF similarity scores between entity addresses</li> </ul> </li> <li>link: Contains match information between entities<ul> <li><code>{entity1}_{entity2}</code>: Links between entities with match scores</li> </ul> </li> <li>User-defined schemas: Contains the original data with cleaned fields<ul> <li>Tables as defined in your configuration</li> </ul> </li> </ol>"},{"location":"output-db-schema/#key-tables","title":"Key Tables","text":""},{"location":"output-db-schema/#entityname","title":"entity.name","text":"<ul> <li><code>entity</code>: Standardized entity name</li> <li><code>name_id</code>: Unique identifier for the entity name</li> </ul>"},{"location":"output-db-schema/#entityaddress","title":"entity.address","text":"<ul> <li><code>entity</code>: Standardized address</li> <li><code>address_id</code>: Unique identifier for the address</li> </ul>"},{"location":"output-db-schema/#entitystreet","title":"entity.street","text":"<ul> <li><code>entity</code>: Standardized street</li> <li><code>street_id</code>: Unique identifier for the street</li> </ul>"},{"location":"output-db-schema/#entityname_similarity","title":"entity.name_similarity","text":"<ul> <li><code>entity_a</code>: First entity name</li> <li><code>entity_b</code>: Second entity name</li> <li><code>similarity</code>: TF-IDF similarity score (0-1)</li> <li><code>id_a</code>: ID of first entity</li> <li><code>id_b</code>: ID of second entity</li> </ul>"},{"location":"output-db-schema/#entitystreet_name_similarity","title":"entity.street_name_similarity","text":"<ul> <li><code>entity_a</code>: First entity address</li> <li><code>entity_b</code>: Second entity address</li> <li><code>similarity</code>: TF-IDF similarity score (0-1)</li> <li><code>id_a</code>: ID of first entity</li> <li><code>id_b</code>: ID of second entity</li> </ul>"},{"location":"output-db-schema/#linkentity1_entity2","title":"link.{entity1}_{entity2}","text":"<ul> <li><code>{entity1}_{id1}</code>: ID from first entity</li> <li><code>{entity2}_{id2}</code>: ID from second entity</li> <li>Various match columns with binary (0/1) or similarity scores (0-1)</li> </ul>"}]}